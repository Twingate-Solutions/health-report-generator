{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e8Dor7X7IHDp"
   },
   "source": [
    "## Intro\n",
    "\n",
    "This notebook generates an xlsx report providing insights into your Twingate environment from Network Events across all Remote Networks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Required Python Dependencies\n",
    "\n",
    "This notebook uses dataframe and polars in order to process large network event datasets and outputs a xlsx spreadsheet, it therefore requires some very specific python libraries to work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nXI-WvVs7uYY",
    "outputId": "cc2b8279-722e-4c5f-ecfd-f5e31abd8e9e",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mDEPRECATION: Configuring installation scheme with distutils config files is deprecated and will no longer work in the near future. If you are using a Homebrew or Linuxbrew Python, please see discussion at https://github.com/Homebrew/homebrew-core/issues/76621\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: xlsxwriter in /opt/homebrew/lib/python3.9/site-packages (3.2.0)\n",
      "\u001b[33mWARNING: Error parsing dependencies of bleach: Expected matching RIGHT_PARENTHESIS for LEFT_PARENTHESIS, after version specifier\n",
      "    tinycss2 (>=1.1.0<1.2) ; extra == 'css'\n",
      "             ~~~~~~~~^\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mDEPRECATION: Configuring installation scheme with distutils config files is deprecated and will no longer work in the near future. If you are using a Homebrew or Linuxbrew Python, please see discussion at https://github.com/Homebrew/homebrew-core/issues/76621\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3.9 -m pip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "\u001b[33mDEPRECATION: Configuring installation scheme with distutils config files is deprecated and will no longer work in the near future. If you are using a Homebrew or Linuxbrew Python, please see discussion at https://github.com/Homebrew/homebrew-core/issues/76621\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: pandas in /opt/homebrew/lib/python3.9/site-packages (1.4.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /opt/homebrew/lib/python3.9/site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/homebrew/lib/python3.9/site-packages (from pandas) (2022.1)\n",
      "Requirement already satisfied: numpy>=1.20.0 in /opt/homebrew/lib/python3.9/site-packages (from pandas) (1.22.4)\n",
      "Requirement already satisfied: six>=1.5 in /opt/homebrew/lib/python3.9/site-packages (from python-dateutil>=2.8.1->pandas) (1.16.0)\n",
      "\u001b[33mWARNING: Error parsing dependencies of bleach: Expected matching RIGHT_PARENTHESIS for LEFT_PARENTHESIS, after version specifier\n",
      "    tinycss2 (>=1.1.0<1.2) ; extra == 'css'\n",
      "             ~~~~~~~~^\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mDEPRECATION: Configuring installation scheme with distutils config files is deprecated and will no longer work in the near future. If you are using a Homebrew or Linuxbrew Python, please see discussion at https://github.com/Homebrew/homebrew-core/issues/76621\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3.9 -m pip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "\u001b[33mDEPRECATION: Configuring installation scheme with distutils config files is deprecated and will no longer work in the near future. If you are using a Homebrew or Linuxbrew Python, please see discussion at https://github.com/Homebrew/homebrew-core/issues/76621\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: polars in /opt/homebrew/lib/python3.9/site-packages (1.17.1)\n",
      "\u001b[33mWARNING: Error parsing dependencies of bleach: Expected matching RIGHT_PARENTHESIS for LEFT_PARENTHESIS, after version specifier\n",
      "    tinycss2 (>=1.1.0<1.2) ; extra == 'css'\n",
      "             ~~~~~~~~^\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mDEPRECATION: Configuring installation scheme with distutils config files is deprecated and will no longer work in the near future. If you are using a Homebrew or Linuxbrew Python, please see discussion at https://github.com/Homebrew/homebrew-core/issues/76621\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3.9 -m pip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# Install python dependencies required\n",
    "%pip install xlsxwriter\n",
    "%pip install pandas\n",
    "%pip install polars"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Specify the Network Event Report to process\n",
    "\n",
    "Change the cell below and specify the full path to the CSV extracted from your Admin Console along with the output xlsx file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "id": "4N7_mjfH_T0g"
   },
   "outputs": [],
   "source": [
    "# specify the timezone you are in, it will become visible in the dataframe and can be used to filter activities based on localtime\n",
    "local_tz = 'America/Los_Angeles'\n",
    "\n",
    "# specify the full path to the Network Events report\n",
    "full_console_report = '/Users/brendansapience/Downloads/202504_cyera_RawNetworkEvents.csv'\n",
    "\n",
    "# specify the full path to the finalized report you'd like to create\n",
    "workbookName = '/Users/brendansapience/Downloads/cyera_insight_report.xlsx'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z-je1RV8JRC2"
   },
   "source": [
    "# Load up the parsing functions below\n",
    "\n",
    "The functions in the following cell are used to generate the final report, they can be combined and used together as well to produce more insights as needed by Admins.\n",
    "\n",
    "The cell below only loads functions in memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "id": "4rp2jWMXz4ql"
   },
   "outputs": [],
   "source": [
    "import json,requests,re\n",
    "import pandas as pd\n",
    "from types import SimpleNamespace\n",
    "import datetime, time, logging, sys, base64, math\n",
    "from urllib.request import urlopen\n",
    "from json import load\n",
    "\n",
    "################################################################\n",
    "# General functions used throughout the notebook\n",
    "################################################################\n",
    "\n",
    "# converts an admin console Network Event reports (csv file) to a flattened normalized dataframe\n",
    "def convert_admin_console_report_to_df(conn_output,tz=None):\n",
    "    df = pl.read_csv(conn_output)\n",
    "    df = df.rename({\"device_id\":\"device.id\",\"start_time\" : \"timestamp.readable\",\"status\" : \"connection.error_message\",\"resource_domain\" : \"resource.address\",\"applied_rule\" : \"resource.applied_rule\",\"bytes_received\" : \"connection.rx\",\"bytes_transferred\" : \"connection.tx\",\"resource_id\" : \"resource.id\",\"remote_network\" : \"remote_network.name\",\"remote_network_id\" : \"remote_network.id\",\"protocol\" : \"connection.protocol\",\"resource_port\" : \"connection.resource_port\",\"resource_ip\" : \"connection.resource_ip\",\"connector_id\" : \"connector.id\",\"user\": \"user.email\", \"user_id\": \"user.id\", \"client_ip\" : \"connection.client_ip\",\"connector\" : \"connector.name\"})\n",
    "    df = df.rename({\"relay_ips\":\"relay.ips\",\"relay_ports\" : \"relay.ports\"})\n",
    "    df = df.rename({\"service_account_id\":\"service_account.id\",\"service_account_key_id\" : \"service_account_key.id\"})\n",
    "    df = df.rename({\"service_account\":\"service_account.name\",\"service_account_key\" : \"service_account_key.name\"})\n",
    "    df = df.drop(['end_time'])\n",
    "    df = df.with_columns(pl.col(\"timestamp.readable\").str.replace(r\" UTC\", \"\"))\n",
    "    df = df.with_columns(pl.col('timestamp.readable').str.strptime(pl.Datetime, \"%Y-%m-%d %H:%M:%S%.f\").dt.convert_time_zone(tz))\n",
    "    df = df.with_columns(pl.format(\"ConnectorId:{}\", \"connector.id\").str.encode(\"base64\").name.keep())\n",
    "    df = df.with_columns(pl.format(\"RemoteNetworkId:{}\", \"remote_network.id\").str.encode(\"base64\").name.keep())\n",
    "    df = df.with_columns(pl.format(\"ResourceId:{}\", \"resource.id\").str.encode(\"base64\").name.keep())\n",
    "    df = df.with_columns(pl.format(\"UserId:{}\", \"user.id\").str.encode(\"base64\").name.keep())\n",
    "    df = df.with_columns(pl.format(\"DeviceId:{}\", \"device.id\").str.encode(\"base64\").name.keep())\n",
    "    df = df.with_columns(pl.col(\"connection.error_message\")).fill_nan('NORMAL')\n",
    "    df = df.with_columns((pl.col(\"timestamp.readable\").dt.strftime('%Y-%m-%d')).alias(\"timestamp.yymmdd\"))\n",
    "    df = df.with_columns(\n",
    "        pl.col(\"resource.address\").fill_null(pl.col(\"connection.resource_ip\"))\n",
    "    )\n",
    "    return df\n",
    "\n",
    "def get_endpoints_from_resource_activity2(df,resource):\n",
    "    logging.debug(\"getting list of activities for a given resource.\")\n",
    "    adf = df.filter(pl.col(\"resource.applied_rule\") == resource)\n",
    "    ResAddresses = adf[\"resource.address\"].unique()\n",
    "    IpAddresses = adf[\"connection.resource_ip\"].unique()\n",
    "    return adf[\"resource.address\"].unique(),adf[\"connection.resource_ip\"].unique()\n",
    "\n",
    "def get_serviceaccount_activity2(df,sa_name):\n",
    "    logging.debug(\"getting all records for a given service account.\")\n",
    "    return df.filter(pl.col(\"service_account.name\") == sa_name)\n",
    "\n",
    "def get_serviceaccountkey_activity2(df,sakey_name):\n",
    "    logging.debug(\"getting all records for a given service account key.\")\n",
    "    return df.filter(pl.col(\"service_account_key.name\") == sakey_name)\n",
    "\n",
    "def get_user_activity2(df,user):\n",
    "    logging.debug(\"getting all records for a given user.\")\n",
    "    return df.filter(pl.col(\"user.email\") == user)\n",
    "\n",
    "def get_user_client_ips2(df,user):\n",
    "    logging.debug(\"getting all records for a given user.\")\n",
    "    df1 = df.filter(pl.col(\"user.email\") == user)\n",
    "    return df1[\"connection.client_ip\"].unique()\n",
    "    \n",
    "def get_errors2(df):\n",
    "    return df.filter(pl.col(\"connection.error_message\") != \"NORMAL\")\n",
    "\n",
    "def get_failures2(df):\n",
    "    if \"event_type\" in df:\n",
    "        return df.filter(pl.col(\"event_type\") == \"failed_to_connect\")\n",
    "\n",
    "def get_relay_vs_p2p_traffic2(df):\n",
    "    p2p_events = df.filter(pl.col(\"relay.ips\").is_null())\n",
    "    relay_events = df.filter(pl.col(\"relay.ips\").is_not_null())\n",
    "    return p2p_events,relay_events\n",
    "\n",
    "def initiate_logging():\n",
    "    root = logging.getLogger()\n",
    "    root.setLevel(logging.INFO)\n",
    "    handler = logging.StreamHandler(sys.stdout)\n",
    "    handler.setLevel(logging.INFO)\n",
    "    formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "    handler.setFormatter(formatter)\n",
    "    root.addHandler(handler)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-fcVwLH2P41t"
   },
   "source": [
    "# Generate the final (XLSX) Report\n",
    "\n",
    "You can customize the output report below in many ways:\n",
    "\n",
    "* Change the title of each Tab\n",
    "* Activate or deactivate sections of the report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "C9VBI_V4Qbt4",
    "outputId": "2982d98e-d17b-48f7-a19e-2e2c9ea5585e",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting Network Event report (csv) to a DataFrame\n",
      "\t => Report has been successfully converted.\n",
      "\n",
      "Extracting Usage Data per User..\n",
      "\t => done.\n",
      "Extracting User and Public IPs..\n",
      "\t => done.\n",
      "Extracting Usage Data per Service Account..\n",
      "\t => done.\n",
      "Extracting Usage Data per Service Account Key..\n",
      "\t => done.\n",
      "Extracting Resource data..\n",
      "Extracting Error Report..\n",
      "\t => done.\n",
      "Extracting Resource Matching data..\n",
      "\t => done.\n",
      "Extracting DNS Errors..\n",
      "\t => done.\n",
      "Extracting Connection Errors..\n",
      "\t => done.\n",
      "Extracting Overall Connector Stats..\n",
      "\t => done.\n",
      "Extracting Per Connector Report..\n",
      "\t => done for Connector silver-lemming\n",
      "\t => done for Connector certain-tarsier\n",
      "\t => done for Connector maroon-cassowary\n",
      "\t => done for Connector judicious-bull\n",
      "\t => done for Connector copper-eel\n",
      "\t => done for Connector warping-raccoon\n",
      "\t => done for Connector scrupulous-goat\n",
      "\t => done for Connector cornflower-mandrill\n",
      "\t => done for Connector olive-galago\n",
      "\t => done for Connector shapeless-dalmatian\n",
      "\t => done for Connector thistle-sheep\n",
      "\t => done for Connector bulky-basilisk\n",
      "\t => done for Connector aquamarine-polecat\n",
      "\t => done for Connector tricky-kingfisher\n",
      "\t => done for Connector strange-kangaroo\n",
      "\t => done for Connector stoic-echidna\n",
      "\t => done for Connector taupe-avocet\n",
      "\t => done for Connector practical-frigatebird\n",
      "\t => done for Connector sage-rattlesnake\n",
      "\t => done for Connector huge-boobook\n",
      "\t => done for Connector independent-ermine\n",
      "\t => done for Connector abiding-lizard\n",
      "\t => done for Connector orange-shark\n",
      "\t => done for Connector ginger-grebe\n",
      "\t => done for Connector fine-beagle\n",
      "\t => done for Connector tuscan-bullfinch\n",
      "\t => done for Connector little-corgi\n",
      "\t => done for Connector rigorous-toad\n",
      "\t => done for Connector winged-taipan\n",
      "\t => done for Connector cherubic-zebu\n",
      "\t => done for Connector cherubic-sponge\n",
      "\t => done for Connector solid-silkworm\n",
      "\t => done for Connector futuristic-cricket\n",
      "\t => done for Connector lovely-firefly\n",
      "\t => done for Connector viridian-hoatzin\n",
      "\t => done for Connector precise-starfish\n",
      "\t => done for Connector curious-warthog\n",
      "\t => done for Connector understanding-cheetah\n",
      "\t => done for Connector understanding-butterfly\n",
      "\t => done for Connector powerful-corgi\n",
      "\t => done for Connector maize-orangutan\n",
      "\t => done for Connector eminent-manul\n",
      "\t => done for Connector pragmatic-crow\n",
      "\t => done for Connector bronze-termite\n",
      "\t => done for Connector papaya-elk\n",
      "\t => done for Connector elastic-coyote\n",
      "\t => done for Connector lyrical-buffalo\n",
      "\t => done for Connector true-ladybug\n",
      "\t => done for Connector cute-mule\n",
      "\t => done for Connector unnatural-caracal\n",
      "\t => done for Connector bright-kiwi\n",
      "\t => done for Connector nifty-polecat\n",
      "\t => done for Connector vague-dodo\n",
      "\t => done for Connector purring-quokka\n",
      "\t => done for Connector azure-pronghorn\n",
      "\t => done for Connector precious-marmoset\n",
      "\t => done for Connector rough-trogon\n",
      "\t => done for Connector quirky-badger\n",
      "\t => done for Connector invisible-dolphin\n",
      "\t => done for Connector mega-skylark\n",
      "\t => done for Connector literate-shark\n",
      "\t => done for Connector outstanding-bird\n",
      "\t => done for Connector debonair-mink\n",
      "\t => done for Connector diligent-potoo\n",
      "\t => done for Connector stylish-waxbill\n",
      "\t => done for Connector optimal-cat\n",
      "\t => done for Connector big-earthworm\n",
      "\t => done for Connector outrageous-civet\n",
      "\t => done for Connector holistic-hedgehog\n",
      "\t => done for Connector courageous-piculet\n",
      "\t => done for Connector satisfied-chamois\n",
      "\t => done for Connector mighty-oyster\n",
      "\t => done for Connector polite-malamute\n",
      "\t => done for Connector statuesque-iguana\n",
      "\t => done for Connector cream-jaguar\n",
      "\t => done for Connector burgundy-marten\n",
      "\t => done for Connector proud-pogona\n",
      "Adding activity summary Tab.\n",
      "\n",
      "Adding IP information Tab.\n",
      "Adding Service Account activity summary Tab.\n",
      "Adding Service Account Key activity summary Tab.\n",
      "Adding resource summary Tab.\n",
      "Adding resource match summary Tab.\n",
      "Adding DNS error summary Tab.\n",
      "Adding Connection error summary Tab.\n",
      "Adding connector summary Tab.\n",
      "Adding per Connector stats summary Tab.\n",
      "\n",
      "Report Generation completed. It took 73.93 seconds to generate.\n"
     ]
    }
   ],
   "source": [
    "import polars as pl\n",
    "import time\n",
    "import json,requests,re\n",
    "from types import SimpleNamespace\n",
    "import datetime, time, logging, sys, base64, math\n",
    "from zoneinfo import ZoneInfo\n",
    "from urllib.request import urlopen\n",
    "from json import load\n",
    "from xlsxwriter import Workbook\n",
    "\n",
    "doUserIPInfo = True # Export User/IP Info\n",
    "userIPSummaryTitle = \"User IP Details\" # Title for Workbook Tab\n",
    "\n",
    "doUserActInfo = True # Export User Activity Details\n",
    "userActTitle = \"User Activity Details\" # Title for Workbook Tab\n",
    "\n",
    "doServiceAccountInfo = True\n",
    "SATitle = \"SA Activity Details\" # Title for Workbook Tab\n",
    "\n",
    "doServiceAccountKeyInfo = True\n",
    "SAKTitle = \"SA Key Activity Details\" # Title for Workbook Tab\n",
    "\n",
    "doResInfo = True # Export Full Resource List\n",
    "doResTitle = \"Full Resource List\" # Title for Workbook Tab\n",
    "\n",
    "doMatchList = True # Export Match List for Wildcard Resources\n",
    "doMatchTitle = \"Resource Matching List\" # Title for Workbook Tab\n",
    "\n",
    "doConnectorsStats = True # Export Overall Connectors Stats\n",
    "doConnectorsStatsTitle = \"Connector Activities\" # Title for Workbook Tab\n",
    "\n",
    "doConnectorsStatsPerConnector = True # Export Per Connectors Stats\n",
    "\n",
    "doConnErrorsStats = True # Export Connection Error Stats\n",
    "doConnErrorsStatsTitle = \"Connection Errors\" # Title for Workbook Tab\n",
    "\n",
    "doDNSErrorsStats = True # Export DNS Error Stats\n",
    "doDNSErrorsStatsTitle = \"DNS Errors\" # Title for Workbook Tab\n",
    "\n",
    "# initiate logging\n",
    "initiate_logging()\n",
    "startTime = time.time()\n",
    "\n",
    "# Convert entire report output to DataFrame for processing.\n",
    "print(\"Converting Network Event report (csv) to a DataFrame\")\n",
    "df = convert_admin_console_report_to_df(full_console_report,local_tz)\n",
    "print(\"\\t => Report has been successfully converted.\\n\")\n",
    "\n",
    "if (doUserIPInfo or doUserActInfo):\n",
    "    unique_users = df[\"user.email\"].unique()\n",
    "\n",
    "    \n",
    "# End User activity tab\n",
    "if (doUserActInfo):\n",
    "    print(\"Extracting Usage Data per User..\")\n",
    "    user_activity_data = {\n",
    "        \"user\":[],\n",
    "        \"numconn\":[],\n",
    "        \"numerr\":[],\n",
    "        \"txbytes\":[],\n",
    "        \"rxbytes\":[],\n",
    "        \"p2p\":[],\n",
    "        \"relay\":[],\n",
    "        \"nbofdevices\":[],\n",
    "        \"lastactivity\":[]\n",
    "    }\n",
    "    user_activity_schema = {\n",
    "        \"user\":pl.String, \n",
    "        \"numconn\":pl.Int32, \n",
    "        \"numerr\":pl.Int32, \n",
    "        \"txbytes\":pl.Int64, \n",
    "        \"rxbytes\": pl.Int64, \n",
    "        \"p2p\":pl.Float32, \n",
    "        \"relay\":pl.Float32,\n",
    "        \"nbofdevices\":pl.Int32,\n",
    "        \"lastactivity\":pl.String\n",
    "    }\n",
    "    \n",
    "    user_activity_df = pl.DataFrame(user_activity_data, schema=user_activity_schema)\n",
    "    for user in unique_users:\n",
    "        if user != None:\n",
    "            user_act = get_user_activity2(df,user)\n",
    "\n",
    "            # last user activity\n",
    "            lastactivity = user_act.tail(1)\n",
    "            lastactivitytimestamp = lastactivity['timestamp.readable']\n",
    "\n",
    "            # list of active user devices\n",
    "            devices = user_act['device.id'].unique()\n",
    "            \n",
    "            # get all P2P & Relay events\n",
    "            p2p,relays = get_relay_vs_p2p_traffic2(user_act)\n",
    "            nb_p2p_events = len(p2p)\n",
    "            nb_relay_events = len(relays)\n",
    "            nb_total_events = nb_p2p_events + nb_relay_events\n",
    "            if nb_total_events > 0:\n",
    "                percentage_p2p = round(nb_p2p_events / nb_total_events * 100,2)\n",
    "                percentage_relay = round(nb_relay_events / nb_total_events * 100,2)\n",
    "            else:\n",
    "                percentage_p2p = 0\n",
    "                percentage_relay = 0\n",
    "\n",
    "            # get all user errors\n",
    "            user_err = get_errors2(user_act)\n",
    "\n",
    "            # get cummulative bandwidth data            \n",
    "            df_temp = user_act.with_columns(pl.col(\"connection.tx\").cast(pl.Int64))\n",
    "            total_tx = df_temp[\"connection.tx\"].sum()\n",
    "            \n",
    "            df_temp = user_act.with_columns(pl.col(\"connection.rx\").cast(pl.Int64))\n",
    "            total_rx = df_temp[\"connection.rx\"].sum()\n",
    "            \n",
    "            temp_act_data = {\n",
    "             \"user\": user, \n",
    "             \"numconn\":len(user_act), \n",
    "             \"numerr\": len(user_err), \n",
    "             \"txbytes\":total_tx, \n",
    "             \"rxbytes\":total_rx, \n",
    "             \"p2p\": percentage_p2p, \n",
    "             \"relay\": percentage_relay, \n",
    "             \"nbofdevices\":len(devices),\n",
    "             \"lastactivity\": lastactivitytimestamp\n",
    "            }\n",
    "\n",
    "            temp_act_df = pl.DataFrame(temp_act_data, schema=user_activity_schema)\n",
    "            user_activity_df = pl.concat([user_activity_df, temp_act_df])\n",
    "\n",
    "    user_activity_df = user_activity_df.rename(\n",
    "        {\n",
    "            \"user\":\"User Email\",\n",
    "            \"numconn\" : \"Nb of Connections\",\n",
    "            \"numerr\" : \"Nb of Errors\",\n",
    "            \"txbytes\" : \"TX (bytes)\",\n",
    "            \"rxbytes\":\"RX (bytes)\",\n",
    "            \"p2p\":\"P2P Connections (%)\",\n",
    "            \"relay\":\"Relay Connections (%)\",\n",
    "            \"nbofdevices\":\"Number of Active Devices\",\n",
    "            \"lastactivity\": \"Last Activity\"\n",
    "        }\n",
    "    )\n",
    "    print(\"\\t => done.\")\n",
    "\n",
    "# End User Public IP tab\n",
    "if (doUserIPInfo):\n",
    "    \n",
    "    print(\"Extracting User and Public IPs..\")\n",
    "\n",
    "    user_ip_info_data = {\n",
    "        \"user\":[],\n",
    "        \"ipinfo\":[],\n",
    "        \"nbofips\":[]\n",
    "    }\n",
    "    \n",
    "    user_ip_info_schema = {\n",
    "        \"user\":pl.String, \n",
    "        \"ipinfo\":pl.String,\n",
    "        \"nbofips\":pl.Int32\n",
    "    }\n",
    "\n",
    "    user_ip_info_df = pl.DataFrame(user_ip_info_data, schema=user_ip_info_schema)\n",
    "\n",
    "    for user in unique_users:\n",
    "        if user != None:\n",
    "            ips = get_user_client_ips2(df,user)\n",
    "            iplist = []\n",
    "            for ip in ips:\n",
    "                iplist.append(str(ip))\n",
    "                ip_info_for_user = \",\".join(iplist)\n",
    "\n",
    "            temp_ip_summary_data = {\n",
    "                \"user\": user, \n",
    "                \"ipinfo\":ip_info_for_user,\n",
    "                \"nbofips\":len(ips)\n",
    "            }\n",
    "\n",
    "            temp_ip_summary_df = pl.DataFrame(temp_ip_summary_data, schema=user_ip_info_schema)\n",
    "            user_ip_info_df = pl.concat([user_ip_info_df,temp_ip_summary_df])\n",
    "\n",
    "    user_ip_info_df = user_ip_info_df.rename(\n",
    "        {\n",
    "            \"user\":\"User Email\",\n",
    "            \"ipinfo\" : \"Public IPs\",\n",
    "            \"nbofips\":\"Total Number of Public IPs\"\n",
    "        }\n",
    "    )\n",
    "    print(\"\\t => done.\")\n",
    "\n",
    "\n",
    "# Service Account activity tab\n",
    "if (doServiceAccountInfo):\n",
    "    print(\"Extracting Usage Data per Service Account..\")\n",
    "    sa_activity_data = {\n",
    "        \"saname\":[],\n",
    "        \"numconn\":[],\n",
    "        \"numerr\":[],\n",
    "        \"txbytes\":[],\n",
    "        \"rxbytes\":[],\n",
    "        \"p2p\":[],\n",
    "        \"relay\":[],\n",
    "        \"lastactivity\":[]\n",
    "    }\n",
    "    sa_activity_schema = {\n",
    "        \"saname\":pl.String, \n",
    "        \"numconn\":pl.Int32, \n",
    "        \"numerr\":pl.Int32, \n",
    "        \"txbytes\":pl.Int64, \n",
    "        \"rxbytes\": pl.Int64, \n",
    "        \"p2p\":pl.Float32, \n",
    "        \"relay\":pl.Float32,\n",
    "        \"lastactivity\":pl.String\n",
    "    }\n",
    "    \n",
    "    unique_sas = df[\"service_account.name\"].unique()\n",
    "    \n",
    "    sa_activity_df = pl.DataFrame(sa_activity_data, schema=sa_activity_schema)\n",
    "    for sa in unique_sas:\n",
    "        if sa != None:\n",
    "            sa_act = get_serviceaccount_activity2(df,sa)\n",
    "\n",
    "            # last SA activity\n",
    "            lastactivity = sa_act.tail(1)\n",
    "            lastactivitytimestamp = lastactivity['timestamp.readable']\n",
    "\n",
    "            # get all P2P & Relay events\n",
    "            p2p,relays = get_relay_vs_p2p_traffic2(sa_act)\n",
    "            nb_p2p_events = len(p2p)\n",
    "            nb_relay_events = len(relays)\n",
    "            nb_total_events = nb_p2p_events + nb_relay_events\n",
    "            if nb_total_events > 0:\n",
    "                percentage_p2p = round(nb_p2p_events / nb_total_events * 100,2)\n",
    "                percentage_relay = round(nb_relay_events / nb_total_events * 100,2)\n",
    "            else:\n",
    "                percentage_p2p = 0\n",
    "                percentage_relay = 0\n",
    "\n",
    "            # get all user errors\n",
    "            sa_err = get_errors2(sa_act)\n",
    "\n",
    "            # get cummulative bandwidth data            \n",
    "            df_temp = sa_act.with_columns(pl.col(\"connection.tx\").cast(pl.Int64))\n",
    "            total_tx = df_temp[\"connection.tx\"].sum()\n",
    "            \n",
    "            df_temp = sa_act.with_columns(pl.col(\"connection.rx\").cast(pl.Int64))\n",
    "            total_rx = df_temp[\"connection.rx\"].sum()\n",
    "            \n",
    "            temp_act_data = {\n",
    "             \"saname\": sa, \n",
    "             \"numconn\":len(sa_act), \n",
    "             \"numerr\": len(sa_err), \n",
    "             \"txbytes\":total_tx, \n",
    "             \"rxbytes\":total_rx, \n",
    "             \"p2p\": percentage_p2p, \n",
    "             \"relay\": percentage_relay, \n",
    "             \"lastactivity\": lastactivitytimestamp\n",
    "            }\n",
    "\n",
    "            temp_act_df = pl.DataFrame(temp_act_data, schema=sa_activity_schema)\n",
    "            sa_activity_df = pl.concat([sa_activity_df, temp_act_df])\n",
    "\n",
    "    sa_activity_df = sa_activity_df.rename(\n",
    "        {\n",
    "            \"saname\":\"Service Account Name\",\n",
    "            \"numconn\" : \"Nb of Connections\",\n",
    "            \"numerr\" : \"Nb of Errors\",\n",
    "            \"txbytes\" : \"TX (bytes)\",\n",
    "            \"rxbytes\":\"RX (bytes)\",\n",
    "            \"p2p\":\"P2P Connections (%)\",\n",
    "            \"relay\":\"Relay Connections (%)\",\n",
    "            \"lastactivity\": \"Last Activity\"\n",
    "        }\n",
    "    )\n",
    "    print(\"\\t => done.\")\n",
    "\n",
    "\n",
    "# Service Account Key activity tab\n",
    "if (doServiceAccountKeyInfo):\n",
    "    print(\"Extracting Usage Data per Service Account Key..\")\n",
    "    sak_activity_data = {\n",
    "        \"sakeyname\":[],\n",
    "        \"saname\": [], \n",
    "        \"numconn\":[],\n",
    "        \"numerr\":[],\n",
    "        \"txbytes\":[],\n",
    "        \"rxbytes\":[],\n",
    "        \"p2p\":[],\n",
    "        \"relay\":[],\n",
    "        \"lastactivity\":[]\n",
    "    }\n",
    "    sak_activity_schema = {\n",
    "        \"sakeyname\":pl.String, \n",
    "        \"saname\":pl.String, \n",
    "        \"numconn\":pl.Int32, \n",
    "        \"numerr\":pl.Int32, \n",
    "        \"txbytes\":pl.Int64, \n",
    "        \"rxbytes\": pl.Int64, \n",
    "        \"p2p\":pl.Float32, \n",
    "        \"relay\":pl.Float32,\n",
    "        \"lastactivity\":pl.String\n",
    "    }\n",
    "    \n",
    "    unique_saks = df[\"service_account_key.name\"].unique()\n",
    "    \n",
    "    sak_activity_df = pl.DataFrame(sak_activity_data, schema=sak_activity_schema)\n",
    "    for sak in unique_saks:\n",
    "        if sak != None:\n",
    "            sak_act = get_serviceaccountkey_activity2(df,sak)\n",
    "\n",
    "            # last SA activity\n",
    "            lastactivity = sak_act.tail(1)\n",
    "            lastactivitytimestamp = lastactivity['timestamp.readable']\n",
    "            saname = lastactivity['service_account.name']\n",
    "\n",
    "            # get all P2P & Relay events\n",
    "            p2p,relays = get_relay_vs_p2p_traffic2(sak_act)\n",
    "            nb_p2p_events = len(p2p)\n",
    "            nb_relay_events = len(relays)\n",
    "            nb_total_events = nb_p2p_events + nb_relay_events\n",
    "            if nb_total_events > 0:\n",
    "                percentage_p2p = round(nb_p2p_events / nb_total_events * 100,2)\n",
    "                percentage_relay = round(nb_relay_events / nb_total_events * 100,2)\n",
    "            else:\n",
    "                percentage_p2p = 0\n",
    "                percentage_relay = 0\n",
    "\n",
    "            # get all user errors\n",
    "            sak_err = get_errors2(sak_act)\n",
    "\n",
    "            # get cummulative bandwidth data            \n",
    "            df_temp = sak_act.with_columns(pl.col(\"connection.tx\").cast(pl.Int64))\n",
    "            total_tx = df_temp[\"connection.tx\"].sum()\n",
    "            \n",
    "            df_temp = sak_act.with_columns(pl.col(\"connection.rx\").cast(pl.Int64))\n",
    "            total_rx = df_temp[\"connection.rx\"].sum()\n",
    "            \n",
    "            temp_act_data = {\n",
    "             \"sakeyname\": sak, \n",
    "             \"saname\": saname,\n",
    "             \"numconn\":len(sak_act), \n",
    "             \"numerr\": len(sak_err), \n",
    "             \"txbytes\":total_tx, \n",
    "             \"rxbytes\":total_rx, \n",
    "             \"p2p\": percentage_p2p, \n",
    "             \"relay\": percentage_relay, \n",
    "             \"lastactivity\": lastactivitytimestamp\n",
    "            }\n",
    "\n",
    "            temp_act_df = pl.DataFrame(temp_act_data, schema=sak_activity_schema)\n",
    "            sak_activity_df = pl.concat([sak_activity_df, temp_act_df])\n",
    "\n",
    "    sak_activity_df = sak_activity_df.rename(\n",
    "        {\n",
    "            \"sakeyname\":\"Service Account Key Name\",\n",
    "            \"saname\":\"Service Account Name\",\n",
    "            \"numconn\" : \"Nb of Connections\",\n",
    "            \"numerr\" : \"Nb of Errors\",\n",
    "            \"txbytes\" : \"TX (bytes)\",\n",
    "            \"rxbytes\":\"RX (bytes)\",\n",
    "            \"p2p\":\"P2P Connections (%)\",\n",
    "            \"relay\":\"Relay Connections (%)\",\n",
    "            \"lastactivity\": \"Last Activity\"\n",
    "        }\n",
    "    )\n",
    "    print(\"\\t => done.\")\n",
    "\n",
    "    \n",
    "# Overall Resource performance tab\n",
    "if (doResInfo):\n",
    "    print(\"Extracting Resource data..\")\n",
    "    all_errors = get_errors2(df)\n",
    "    unique_addr_in_df = df[\"resource.address\"].unique()\n",
    "    res_data = {\n",
    "        \"address\":[],\n",
    "        \"resourcedef\":[],\n",
    "        \"resourceids\":[],\n",
    "        \"rnetworks\":[],\n",
    "        \"numconn\":[],\n",
    "        \"numerrs\":[],\n",
    "        \"failrate\":[],\n",
    "        \"totaltx\":[],\n",
    "        \"totalrx\":[],\n",
    "        \"tcp\":[],\n",
    "        \"udp\":[],\n",
    "        \"icmp\":[]\n",
    "    }\n",
    "    \n",
    "    res_data_schema = {\n",
    "        \"address\":pl.String, \n",
    "        \"resourcedef\":pl.String, \n",
    "        \"resourceids\":pl.String, \n",
    "        \"rnetworks\":pl.String, \n",
    "        \"numconn\":pl.Int32, \n",
    "        \"numerrs\":pl.Int32, \n",
    "        \"failrate\":pl.Float32, \n",
    "        \"totaltx\":pl.Int64, \n",
    "        \"totalrx\":pl.Int64, \n",
    "        \"tcp\":pl.String, \n",
    "        \"udp\":pl.String, \n",
    "        \"icmp\":pl.String\n",
    "    }\n",
    "    \n",
    "    \n",
    "    \n",
    "    res_data_df = pl.DataFrame(res_data, schema=res_data_schema)\n",
    "    \n",
    "    print(\"Extracting Error Report..\")\n",
    "   \n",
    "    error_data_schema = {\n",
    "        \"address\":pl.String, \n",
    "        \"resourcedef\":pl.String, \n",
    "        \"resourceids\":pl.String, \n",
    "        \"rnetworks\":pl.String, \n",
    "        \"numconn\":pl.Int32, \n",
    "        \"numerrs\":pl.Int32, \n",
    "        \"failrate\":pl.Float32, \n",
    "        \"totaltx\":pl.Int64, \n",
    "        \"totalrx\":pl.Int64, \n",
    "        \"tcp\":pl.String, \n",
    "        \"udp\":pl.String, \n",
    "        \"icmp\":pl.String\n",
    "    }\n",
    "    \n",
    "    for addr in unique_addr_in_df:\n",
    "        if str(addr) != \"nan\":\n",
    "            total_for_addr = df.filter(pl.col(\"resource.address\") == addr)\n",
    "            err_for_addr = all_errors.filter(pl.col(\"resource.address\") == addr)\n",
    "\n",
    "            if len(total_for_addr) > 0:\n",
    "                percentage_failure = round(len(err_for_addr) / len(total_for_addr) * 100,2)\n",
    "            else:\n",
    "                percentage_failure = 100\n",
    "\n",
    "            port_connected = []\n",
    "            addr_activities = df.filter(pl.col(\"resource.address\") == addr)\n",
    "\n",
    "            # pull resource definition, resource ID (as would appear in the Admin Console & API) and Remote Network name\n",
    "            res_definitions = addr_activities[\"resource.applied_rule\"].unique()\n",
    "            res_ids = addr_activities[\"resource.id\"].unique()\n",
    "            rnetwork_names = addr_activities[\"remote_network.name\"].unique()\n",
    "\n",
    "            res_def_list=[]\n",
    "            for definition in res_definitions:\n",
    "                res_def_list.append(definition)\n",
    "            res_def_list_s=\";\".join(str(x) for x in res_def_list)\n",
    "            \n",
    "            res_id_list=[]\n",
    "            for definition in res_ids:\n",
    "                res_id_list.append(definition)\n",
    "            res_id_list_s=\";\".join(str(x) for x in res_id_list)\n",
    "            \n",
    "            res_rnetwork_list=[]\n",
    "            for definition in rnetwork_names:\n",
    "                res_rnetwork_list.append(definition)\n",
    "            res_rnetwork_list_s=\";\".join(str(x) for x in res_rnetwork_list)\n",
    "\n",
    "            df_temp = addr_activities.with_columns(pl.col(\"connection.tx\").cast(pl.Int64))\n",
    "            total_tx = df_temp[\"connection.tx\"].sum()\n",
    "            \n",
    "            df_temp = addr_activities.with_columns(pl.col(\"connection.rx\").cast(pl.Int64))\n",
    "            total_rx = df_temp[\"connection.rx\"].sum()\n",
    "\n",
    "            addr_activities_protocols = addr_activities[\"connection.protocol\"].unique()\n",
    "            addr_activities_icmp = addr_activities.filter(pl.col(\"connection.protocol\") == \"icmp\")\n",
    "            addr_activities_tcp = addr_activities.filter(pl.col(\"connection.protocol\") == \"tcp\")\n",
    "            addr_activities_udp = addr_activities.filter(pl.col(\"connection.protocol\") == \"udp\")\n",
    "\n",
    "            ports_tcp = addr_activities_tcp[\"connection.resource_port\"].unique()\n",
    "            ports_udp = addr_activities_udp[\"connection.resource_port\"].unique()\n",
    "            ports_icmp = addr_activities_icmp[\"connection.resource_port\"].unique()\n",
    "\n",
    "            tcp_port_list = []\n",
    "            udp_port_list = []\n",
    "            icmp_port_list = []\n",
    "            for port in ports_tcp:\n",
    "                tcp_port_list.append(port)\n",
    "            for port in ports_udp:\n",
    "                udp_port_list.append(port)\n",
    "            for port in ports_icmp:\n",
    "                icmp_port_list.append(port)\n",
    "            tcp_port_list_s=\";\".join(str(x) for x in tcp_port_list)\n",
    "            udp_port_list_s=\";\".join(str(x) for x in udp_port_list)\n",
    "            icmp_port_list_s=\";\".join(str(x) for x in icmp_port_list)\n",
    "            \n",
    "            res_data_temp_df = pl.DataFrame(res_data, schema=error_data_schema)\n",
    "\n",
    "\n",
    "            temp_res_data = {\n",
    "                \"address\":str(addr),\n",
    "                \"resourcedef\":str(res_def_list_s),\n",
    "                \"resourceids\":str(res_id_list_s),\n",
    "                \"rnetworks\":str(res_rnetwork_list_s), \n",
    "                \"numconn\":len(total_for_addr), \n",
    "                \"numerrs\":len(err_for_addr), \n",
    "                \"failrate\":percentage_failure, \n",
    "                \"totaltx\":total_tx, \n",
    "                \"totalrx\":total_rx, \n",
    "                \"tcp\":tcp_port_list_s, \n",
    "                \"udp\":udp_port_list_s, \n",
    "                \"icmp\":icmp_port_list_s\n",
    "            }\n",
    "            \n",
    "            temp_res_df = pl.DataFrame(temp_res_data, schema=error_data_schema)\n",
    "            res_data_df = pl.concat([res_data_df, temp_res_df])\n",
    "\n",
    "    res_data_df = res_data_df.rename(\n",
    "        {\n",
    "            \"address\":\"Resource Address\",\n",
    "            \"resourcedef\":\"Resource Definitions\",\n",
    "            \"resourceids\":\"Resource IDs\",\n",
    "            \"rnetworks\":\"Remote Networks\",\n",
    "            \"numconn\" : \"Nb of Connections\",\n",
    "            \"numerrs\" : \"Nb of Errors\",\n",
    "            \"totaltx\" : \"TX (bytes)\",\n",
    "            \"totalrx\":\"RX (bytes)\",\n",
    "            \"failrate\":\"Failure Rate (%)\"\n",
    "        }\n",
    "    )\n",
    "    print(\"\\t => done.\")\n",
    "    \n",
    "# Resource matching list\n",
    "if (doMatchList):\n",
    "    # get the list of unique resource definitions that have caught traffic (with the list of actual addresses hit for each resource)\n",
    "    print(\"Extracting Resource Matching data..\")\n",
    "    unique_resources = df[\"resource.applied_rule\"].unique()\n",
    "    \n",
    "    match_data = {\n",
    "        \"resource\":[],\n",
    "        \"matches\":[]\n",
    "    }\n",
    "    \n",
    "    match_data_schema = {\n",
    "        \"resource\":pl.String, \n",
    "        \"matches\":pl.String\n",
    "    }\n",
    "    \n",
    "    match_data_df = pl.DataFrame(match_data, schema=match_data_schema)\n",
    "    for r in unique_resources:\n",
    "        addresses_for_resource,ips_for_resource = get_endpoints_from_resource_activity2(df,r)\n",
    "        \n",
    "        # if the list of unique addresses returns only 1 address and said address is None, then the source of truth should be IPs and not FQDNs\n",
    "        if len(addresses_for_resource) == 1:\n",
    "            addr_joined=\"; \".join(str(x) for x in addresses_for_resource)\n",
    "            if addr_joined == \"None\":\n",
    "                addr_joined=\"; \".join(str(x) for x in ips_for_resource)\n",
    "        else:\n",
    "            addr_joined=\"; \".join(str(x) for x in addresses_for_resource) \n",
    "                \n",
    "        temp_match_data = {\n",
    "            \"resource\":r, \n",
    "            \"matches\":addr_joined\n",
    "        }\n",
    "        \n",
    "        temp_match_data_df = pl.DataFrame(temp_match_data, schema=match_data_schema)\n",
    "        match_data_df = pl.concat([match_data_df, temp_match_data_df])\n",
    "\n",
    "    match_data_df = match_data_df.rename(\n",
    "        {\n",
    "            \"resource\":\"Resource Definition\",\n",
    "            \"matches\" : \"Corresponding Addresses\"\n",
    "        }\n",
    "    )\n",
    "    print(\"\\t => done.\")\n",
    "\n",
    "# DNS Error tab\n",
    "if(doDNSErrorsStats):\n",
    "    print(\"Extracting DNS Errors..\")\n",
    "\n",
    "    connact_data = {\n",
    "            \"address\":[],\n",
    "            \"nberrors\":[]\n",
    "      }\n",
    "    \n",
    "    connact_data_schema = {\n",
    "        \"address\":pl.String, \n",
    "        \"nberrors\":pl.Int64\n",
    "    }\n",
    "\n",
    "    dns_errors_df = pl.DataFrame(connact_data, schema=connact_data_schema)\n",
    "    activities_dnserror = df.filter(pl.col(\"connection.error_message\") == \"DNS_ERROR\")\n",
    "    unique_dns_failures = activities_dnserror['resource.address'].unique()\n",
    "\n",
    "    for addr in unique_dns_failures:\n",
    "        dns_ct = activities_dnserror.filter(pl.col(\"resource.address\") == addr).select(pl.len()).item()\n",
    "\n",
    "        temp_dns_errors = {\n",
    "            \"address\":addr, \n",
    "            \"nberrors\":dns_ct\n",
    "        }\n",
    "        \n",
    "        temp_dns_errors_df = pl.DataFrame(temp_dns_errors, schema=connact_data_schema)\n",
    "\n",
    "        dns_errors_df = pl.concat([dns_errors_df, temp_dns_errors_df])\n",
    "\n",
    "    dns_errors_df = dns_errors_df.rename(\n",
    "        {\n",
    "            \"address\":\"Resource Address\",\n",
    "            \"nberrors\" : \"Number of DNS Errors\"\n",
    "        }\n",
    "    )\n",
    "    print(\"\\t => done.\")\n",
    "\n",
    "# Connection Error tab\n",
    "if(doConnErrorsStats):\n",
    "    print(\"Extracting Connection Errors..\")\n",
    "\n",
    "    connact_data = {\n",
    "        \"address\":[],\n",
    "        \"nberrors\":[]\n",
    "    }\n",
    "    \n",
    "    connact_data_schema = {\n",
    "        \"address\":pl.String, \n",
    "        \"nberrors\":pl.Int64\n",
    "    }\n",
    "\n",
    "    conn_errors_df = pl.DataFrame(connact_data, schema=connact_data_schema)\n",
    "    activities_connectionfailed = df.filter(pl.col(\"connection.error_message\") == \"CONNECTION_FAILED\")\n",
    "    unique_address_failures = activities_connectionfailed['resource.address'].unique()\n",
    "\n",
    "    for addr in unique_address_failures:\n",
    "        err_ct = activities_connectionfailed.filter(pl.col(\"resource.address\") == addr).select(pl.len()).item()\n",
    "\n",
    "        temp_conn_errors = {\n",
    "            \"address\":addr, \n",
    "            \"nberrors\":err_ct\n",
    "        }\n",
    "        \n",
    "        temp_conn_errors_df = pl.DataFrame(temp_conn_errors, schema=connact_data_schema)\n",
    "        conn_errors_df = pl.concat([conn_errors_df, temp_conn_errors_df])\n",
    "\n",
    "    conn_errors_df = conn_errors_df.rename(\n",
    "        {\n",
    "            \"address\":\"Resource Address\",\n",
    "            \"nberrors\" : \"Number of Connection Errors\"\n",
    "        }\n",
    "    )\n",
    "    print(\"\\t => done.\")\n",
    "    \n",
    "# Overall Connector performance tab\n",
    "if (doConnectorsStats):\n",
    "    print(\"Extracting Overall Connector Stats..\")\n",
    "    unique_connectors = df[\"connector.name\"].unique()\n",
    "    \n",
    "    connact_data = {\n",
    "        \"connectorname\":[],\n",
    "        \"rnname\":[],\n",
    "        \"totalactivities\":[],\n",
    "        \"connsuccess\":[],\n",
    "        \"connerror\":[],\n",
    "        \"dnserror\":[]\n",
    "    }\n",
    "    \n",
    "    connact_data_schema = {\n",
    "        \"connectorname\":pl.String,\n",
    "        \"rnname\":pl.String, \n",
    "        \"totalactivities\":pl.Int64, \n",
    "        \"connsuccess\":pl.Int64, \n",
    "        \"connerror\":pl.Int64, \n",
    "        \"dnserror\":pl.Int64\n",
    "    }\n",
    "    \n",
    "    connector_data_df = pl.DataFrame(connact_data, schema=connact_data_schema)\n",
    "    \n",
    "    for connector in unique_connectors:\n",
    "        \n",
    "        conn_activities = df.filter(pl.col(\"connector.name\") == connector)\n",
    "        rn_name = conn_activities['remote_network.name'].unique()[0]\n",
    "        conn_activities_normal = conn_activities.filter(pl.col(\"connection.error_message\") == \"NORMAL\")\n",
    "        conn_activities_connectionfailed = conn_activities.filter(pl.col(\"connection.error_message\") == \"CONNECTION_FAILED\")\n",
    "        conn_activities_dnserror = conn_activities.filter(pl.col(\"connection.error_message\") == \"DNS_ERROR\")\n",
    "\n",
    "        temp_res_data = {\n",
    "            \"connectorname\":str(connector),\n",
    "            \"rnname\":str(rn_name), \n",
    "            \"totalactivities\":len(conn_activities),\n",
    "            \"connsuccess\":len(conn_activities_normal),\n",
    "            \"connerror\":len(conn_activities_connectionfailed),\n",
    "            \"dnserror\":len(conn_activities_dnserror)\n",
    "        }\n",
    "        temp_res_df = pl.DataFrame(temp_res_data, schema=connact_data_schema)\n",
    "\n",
    "        connector_data_df = pl.concat([connector_data_df, temp_res_df])\n",
    "\n",
    "    connector_data_df = connector_data_df.rename(\n",
    "        {\n",
    "            \"connectorname\":\"Connector Name\",\n",
    "            \"totalactivities\" : \"Nb of Activities\",\n",
    "            \"rnname\":\"Remote Network Name\",\n",
    "            \"connsuccess\":\"Successful Connections\",\n",
    "            \"connerror\":\"Failed Connections\",\n",
    "            \"dnserror\":\"DNS Errors\"\n",
    "        }\n",
    "    )\n",
    "    print(\"\\t => done.\")\n",
    "    \n",
    "# Per Connector performance tab\n",
    "if (doConnectorsStatsPerConnector):\n",
    "    print(\"Extracting Per Connector Report..\")\n",
    "    unique_connectors = df[\"connector.name\"].unique()\n",
    "    \n",
    "    connact_data = {\n",
    "        \"date\":[],\n",
    "        \"connectorname\":[],\n",
    "        \"rnname\":[],\n",
    "        \"totalactivities\":[],\n",
    "        \"connsuccess\":[],\n",
    "        \"connerror\":[],\n",
    "        \"dnserror\":[]\n",
    "    }\n",
    "    \n",
    "    connact_data_scheme = {\n",
    "        \"date\":pl.String,\n",
    "        \"connectorname\":pl.String,\n",
    "        \"rnname\":pl.String, \n",
    "        \"totalactivities\":pl.Int64, \n",
    "        \"connsuccess\":pl.Int64, \n",
    "        \"connerror\":pl.Int64, \n",
    "        \"dnserror\":pl.Int64\n",
    "    }\n",
    "\n",
    "    all_connector_data = {}\n",
    "    for connector in unique_connectors:\n",
    "        per_connector_data_df = pl.DataFrame(connact_data, schema=connact_data_scheme)\n",
    "        conn_activities = df.filter(pl.col(\"connector.name\") == connector)\n",
    "        rn_name = conn_activities['remote_network.name'].unique()[0]\n",
    "        unique_days_of_Activities = conn_activities[\"timestamp.yymmdd\"].unique()\n",
    "        for day in unique_days_of_Activities:\n",
    "            conn_day_activities = conn_activities.filter(pl.col(\"timestamp.yymmdd\") == day)\n",
    "            connection_normal = conn_day_activities.filter(pl.col(\"connection.error_message\") == \"NORMAL\")\n",
    "            connection_errors = conn_day_activities.filter(pl.col(\"connection.error_message\") == \"CONNECTION_FAILED\")\n",
    "            dns_errors = conn_day_activities.filter(pl.col(\"connection.error_message\") == \"DNS_ERROR\")\n",
    "\n",
    "            temp_res_data = {\n",
    "                \"date\":str(day),\n",
    "                \"connectorname\":str(connector),\n",
    "                \"rnname\":str(rn_name), \n",
    "                \"totalactivities\":len(conn_day_activities),\n",
    "                \"connsuccess\":len(connection_normal),\n",
    "                \"connerror\":len(connection_errors),\n",
    "                \"dnserror\":len(dns_errors)\n",
    "            }\n",
    "\n",
    "            temp_res_df = pl.DataFrame(temp_res_data, schema=connact_data_scheme)\n",
    "\n",
    "            per_connector_data_df = pl.concat([per_connector_data_df, temp_res_df])\n",
    "\n",
    "        per_connector_data_df = per_connector_data_df.rename(\n",
    "            {\n",
    "                \"connectorname\":\"Connector Name\",\n",
    "                \"totalactivities\" : \"Nb of Activities\",\n",
    "                \"rnname\":\"Remote Network Name\",\n",
    "                \"connsuccess\":\"Successful Connections\",\n",
    "                \"connerror\":\"Failed Connections\",\n",
    "                \"dnserror\":\"DNS Errors\"\n",
    "            }\n",
    "        )\n",
    "        print(\"\\t => done for Connector \"+connector)\n",
    "\n",
    "        all_connector_data[connector] = per_connector_data_df\n",
    "\n",
    "# Generate the XLSX Report with all tabs & data\n",
    "with Workbook(workbookName) as wb:\n",
    "\n",
    "    if(doUserActInfo):\n",
    "        print(\"Adding activity summary Tab.\")\n",
    "        user_activity_df.write_excel(\n",
    "            workbook=wb,\n",
    "            worksheet=userActTitle\n",
    "        )\n",
    "\n",
    "    if(doUserIPInfo):\n",
    "        print(\"\\nAdding IP information Tab.\")\n",
    "        user_ip_info_df.write_excel(\n",
    "            workbook=wb,\n",
    "            worksheet=userIPSummaryTitle\n",
    "        )\n",
    "\n",
    "    if(doServiceAccountInfo):\n",
    "        print(\"Adding Service Account activity summary Tab.\")\n",
    "        sa_activity_df.write_excel(\n",
    "            workbook=wb,\n",
    "            worksheet=SATitle\n",
    "        )\n",
    "        \n",
    "    if(doServiceAccountKeyInfo):\n",
    "        print(\"Adding Service Account Key activity summary Tab.\")\n",
    "        sak_activity_df.write_excel(\n",
    "            workbook=wb,\n",
    "            worksheet=SAKTitle\n",
    "        )\n",
    "        \n",
    "    if(doResInfo):\n",
    "        print(\"Adding resource summary Tab.\")\n",
    "        res_data_df.write_excel(\n",
    "            workbook=wb,\n",
    "            worksheet=doResTitle\n",
    "        )\n",
    "    if(doMatchList):\n",
    "        print(\"Adding resource match summary Tab.\")\n",
    "        match_data_df.write_excel(\n",
    "            workbook=wb,\n",
    "            worksheet=doMatchTitle\n",
    "        )\n",
    "    if(doDNSErrorsStats):\n",
    "        print(\"Adding DNS error summary Tab.\")\n",
    "        dns_errors_df.write_excel(\n",
    "            workbook=wb,\n",
    "            worksheet=doDNSErrorsStatsTitle\n",
    "      )\n",
    "\n",
    "    if(doConnErrorsStats):\n",
    "        print(\"Adding Connection error summary Tab.\")\n",
    "        conn_errors_df.write_excel(\n",
    "            workbook=wb,\n",
    "            worksheet=doConnErrorsStatsTitle\n",
    "      )\n",
    "    if(doConnectorsStats):\n",
    "        print(\"Adding connector summary Tab.\")\n",
    "        connector_data_df.write_excel(\n",
    "        workbook=wb,\n",
    "        worksheet=doConnectorsStatsTitle\n",
    "    )\n",
    "\n",
    "    if(doConnectorsStatsPerConnector):\n",
    "        print(\"Adding per Connector stats summary Tab.\")\n",
    "        #all_connector_data\n",
    "        for key in all_connector_data:\n",
    "            all_connector_data[key].write_excel(\n",
    "                workbook=wb,\n",
    "                worksheet=key\n",
    "            )\n",
    "\n",
    "    for WS in wb.worksheets():\n",
    "        WS.autofit()\n",
    "        #WS.autofilter('A1:Z1000')\n",
    "\n",
    "    endTime = time.time()\n",
    "    timeDiff = endTime - startTime\n",
    "    timeDiff = timeDiff * 10**3/1000\n",
    "    \n",
    "    print(\"\\nReport Generation completed. It took \" + str(round(timeDiff,2)) + \" seconds to generate.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
