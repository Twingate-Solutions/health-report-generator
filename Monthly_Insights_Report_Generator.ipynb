{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e8Dor7X7IHDp"
   },
   "source": [
    "## Intro\n",
    "\n",
    "This notebook generates an xlsx report providing insights into your Twingate environment from Network Events across all Remote Networks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Required Python Dependencies\n",
    "\n",
    "This notebook uses dataframe and polars in order to process large network event datasets and outputs a xlsx spreadsheet, it therefore requires some very specific python libraries to work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nXI-WvVs7uYY",
    "outputId": "cc2b8279-722e-4c5f-ecfd-f5e31abd8e9e",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mDEPRECATION: Configuring installation scheme with distutils config files is deprecated and will no longer work in the near future. If you are using a Homebrew or Linuxbrew Python, please see discussion at https://github.com/Homebrew/homebrew-core/issues/76621\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: xlsxwriter in /opt/homebrew/lib/python3.9/site-packages (3.2.0)\n",
      "\u001b[33mWARNING: Error parsing dependencies of bleach: Expected matching RIGHT_PARENTHESIS for LEFT_PARENTHESIS, after version specifier\n",
      "    tinycss2 (>=1.1.0<1.2) ; extra == 'css'\n",
      "             ~~~~~~~~^\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mDEPRECATION: Configuring installation scheme with distutils config files is deprecated and will no longer work in the near future. If you are using a Homebrew or Linuxbrew Python, please see discussion at https://github.com/Homebrew/homebrew-core/issues/76621\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3.9 -m pip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "\u001b[33mDEPRECATION: Configuring installation scheme with distutils config files is deprecated and will no longer work in the near future. If you are using a Homebrew or Linuxbrew Python, please see discussion at https://github.com/Homebrew/homebrew-core/issues/76621\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: pandas in /opt/homebrew/lib/python3.9/site-packages (1.4.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /opt/homebrew/lib/python3.9/site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/homebrew/lib/python3.9/site-packages (from pandas) (2022.1)\n",
      "Requirement already satisfied: numpy>=1.20.0 in /opt/homebrew/lib/python3.9/site-packages (from pandas) (1.22.4)\n",
      "Requirement already satisfied: six>=1.5 in /opt/homebrew/lib/python3.9/site-packages (from python-dateutil>=2.8.1->pandas) (1.16.0)\n",
      "\u001b[33mWARNING: Error parsing dependencies of bleach: Expected matching RIGHT_PARENTHESIS for LEFT_PARENTHESIS, after version specifier\n",
      "    tinycss2 (>=1.1.0<1.2) ; extra == 'css'\n",
      "             ~~~~~~~~^\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mDEPRECATION: Configuring installation scheme with distutils config files is deprecated and will no longer work in the near future. If you are using a Homebrew or Linuxbrew Python, please see discussion at https://github.com/Homebrew/homebrew-core/issues/76621\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3.9 -m pip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "\u001b[33mDEPRECATION: Configuring installation scheme with distutils config files is deprecated and will no longer work in the near future. If you are using a Homebrew or Linuxbrew Python, please see discussion at https://github.com/Homebrew/homebrew-core/issues/76621\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: polars in /opt/homebrew/lib/python3.9/site-packages (1.17.1)\n",
      "\u001b[33mWARNING: Error parsing dependencies of bleach: Expected matching RIGHT_PARENTHESIS for LEFT_PARENTHESIS, after version specifier\n",
      "    tinycss2 (>=1.1.0<1.2) ; extra == 'css'\n",
      "             ~~~~~~~~^\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mDEPRECATION: Configuring installation scheme with distutils config files is deprecated and will no longer work in the near future. If you are using a Homebrew or Linuxbrew Python, please see discussion at https://github.com/Homebrew/homebrew-core/issues/76621\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3.9 -m pip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# Install python dependencies required\n",
    "%pip install xlsxwriter\n",
    "%pip install pandas\n",
    "%pip install polars"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Specify the Network Event Report to process\n",
    "\n",
    "Change the cell below and specify the full path to the CSV extracted from your Admin Console along with the output xlsx file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "4N7_mjfH_T0g"
   },
   "outputs": [],
   "source": [
    "# specify the timezone you are in, it will become visible in the dataframe and can be used to filter activities based on localtime\n",
    "local_tz = 'America/Los_Angeles'\n",
    "full_console_report = '/Users/brendansapience/Downloads/input_report_from_admin_console.csv'\n",
    "workbookName = '/Users/brendansapience/Downloads/output_insights_report.xlsx'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z-je1RV8JRC2"
   },
   "source": [
    "# Load up the parsing functions below\n",
    "\n",
    "The functions in the following cell are used to generate the final report, they can be combined and used together as well to produce more insights as needed by Admins.\n",
    "\n",
    "The cell below only loads functions in memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "4rp2jWMXz4ql"
   },
   "outputs": [],
   "source": [
    "import json,requests,re\n",
    "import pandas as pd\n",
    "from types import SimpleNamespace\n",
    "import datetime, time, logging, sys, base64, math\n",
    "from urllib.request import urlopen\n",
    "from json import load\n",
    "\n",
    "################################################################\n",
    "\n",
    "# Use the following functions to extract what you need\n",
    "\n",
    "################################################################\n",
    "\n",
    "# converts an admin console connector reports to a flattened normalized DF\n",
    "def convert_admin_console_report_to_df(conn_output,tz=None):\n",
    "    df = pl.read_csv(conn_output)\n",
    "    \n",
    "    df = df.rename({\"device_id\":\"device.id\",\"start_time\" : \"timestamp.readable\",\"status\" : \"connection.error_message\",\"resource_domain\" : \"resource.address\",\"applied_rule\" : \"resource.applied_rule\",\"bytes_received\" : \"connection.rx\",\"bytes_transferred\" : \"connection.tx\",\"resource_id\" : \"resource.id\",\"remote_network\" : \"remote_network.name\",\"remote_network_id\" : \"remote_network.id\",\"protocol\" : \"connection.protocol\",\"resource_port\" : \"connection.resource_port\",\"resource_ip\" : \"connection.resource_ip\",\"connector_id\" : \"connector.id\",\"user\": \"user.email\", \"user_id\": \"user.id\", \"client_ip\" : \"connection.client_ip\",\"connector\" : \"connector.name\"})\n",
    "    df = df.drop(['relays', 'relay_ips','relay_ports','end_time'])\n",
    "    df = df.with_columns(pl.col(\"timestamp.readable\").str.replace(r\" UTC\", \"\"))\n",
    "    df = df.with_columns(pl.col('timestamp.readable').str.strptime(pl.Datetime, \"%Y-%m-%d %H:%M:%S%.f\").dt.convert_time_zone(tz))\n",
    "    df = df.with_columns(pl.format(\"ConnectorId:{}\", \"connector.id\").str.encode(\"base64\").name.keep())\n",
    "    df = df.with_columns(pl.format(\"RemoteNetworkId:{}\", \"remote_network.id\").str.encode(\"base64\").name.keep())\n",
    "    df = df.with_columns(pl.format(\"ResourceId:{}\", \"resource.id\").str.encode(\"base64\").name.keep())\n",
    "    df = df.with_columns(pl.format(\"UserId:{}\", \"user.id\").str.encode(\"base64\").name.keep())\n",
    "    df = df.with_columns(pl.format(\"DeviceId:{}\", \"device.id\").str.encode(\"base64\").name.keep())\n",
    "    df = df.with_columns(pl.col(\"connection.error_message\")).fill_nan('NORMAL')\n",
    "    df = df.with_columns((pl.col(\"timestamp.readable\").dt.strftime('%Y-%m-%d')).alias(\"timestamp.yymmdd\"))\n",
    "    # Jan 31 2024 - resource.address is empty when the resource definition is a cidr block and instead resource_ip os used\n",
    "    # we replace empty values in resource.address with the ip instead\n",
    "    df = df.with_columns(\n",
    "        pl.col(\"resource.address\").fill_null(pl.col(\"connection.resource_ip\"))\n",
    "    )\n",
    "\n",
    "    return df\n",
    "\n",
    "# converts ANALYTICS output into a flattened normalized DF (except for Relay info)\n",
    "# it is obtained by running the following command:\n",
    "# journalctl -u twingate-connector --since \"X min ago\" | grep \"ANALYTICS\" | sed 's/.* ANALYTICS//' | sed 'r/ /\\ /g' > somefile\n",
    "def convert_connector_output_to_df(conn_output,tz=None):\n",
    "    df = pd.DataFrame()\n",
    "    f = open(connector_log, \"r\")\n",
    "    for line in f.readlines():\n",
    "        data = json.loads(line)\n",
    "        df1 = pd.json_normalize(data)\n",
    "        df = pd.concat([df, df1])\n",
    "\n",
    "    # adding a human readable timestamp to each line\n",
    "    df['timestamp.readable'] = df['timestamp'].apply(epoch_to_date,args=(tz,))\n",
    "\n",
    "    # object ids in Connector logs are the internal DB ids which are base64 decoded versions of API Ids.\n",
    "    # converting internal Ids to API Ids in DF\n",
    "    df['connector.id'] = df['connector.id'].apply(convert_id_to_api_id,args=(\"connector\",))\n",
    "    df['remote_network.id'] = df['remote_network.id'].apply(convert_id_to_api_id,args=(\"remotenetwork\",))\n",
    "    df['resource.id'] = df['resource.id'].apply(convert_id_to_api_id,args=(\"resource\",))\n",
    "    df['user.id'] = df['user.id'].apply(convert_id_to_api_id,args=(\"user\",))\n",
    "    df['device.id'] = df['device.id'].apply(convert_id_to_api_id,args=(\"device\",))\n",
    "    df[['connection.error_message']] = df[['connection.error_message']].fillna('NORMAL')\n",
    "    # dropping relay info\n",
    "    df = df.drop(columns=['relays','connection.cbct_freshness'])\n",
    "\n",
    "    return df\n",
    "\n",
    "def get_all_timezones():\n",
    "    zoneinfo.available_timezones()\n",
    "\n",
    "# returns the unique list of addresses connected to by users.\n",
    "def get_address_activity(df,addr):\n",
    "    logging.debug(\"getting list of activities for a given address.\")\n",
    "    return df.loc[df[\"resource.address\"] == addr]\n",
    "\n",
    "def get_resource_activity(df,resource):\n",
    "    logging.debug(\"getting list of activities for a given resource.\")\n",
    "    return df.loc[df[\"resource.applied_rule\"] == resource]\n",
    "\n",
    "def get_endpoints_from_resource_activity(df,resource):\n",
    "    logging.debug(\"getting list of activities for a given resource.\")\n",
    "    adf = df.loc[df[\"resource.applied_rule\"] == resource]\n",
    "    return adf[\"resource.address\"].unique()\n",
    "\n",
    "# Polars alternative function\n",
    "def get_endpoints_from_resource_activity2(df,resource):\n",
    "    logging.debug(\"getting list of activities for a given resource.\")\n",
    "    adf = df.filter(pl.col(\"resource.applied_rule\") == resource)\n",
    "    #connection.resource_ip\n",
    "    ResAddresses = adf[\"resource.address\"].unique()\n",
    "    IpAddresses = adf[\"connection.resource_ip\"].unique()\n",
    "    return adf[\"resource.address\"].unique(),adf[\"connection.resource_ip\"].unique()\n",
    "\n",
    "def get_unique_addresses(df):\n",
    "    logging.debug(\"getting unique list of endpoints hit by clients.\")\n",
    "    return df[\"resource.address\"].unique()\n",
    "\n",
    "def get_unique_resources(df):\n",
    "    logging.debug(\"getting unique list of resources hit by clients.\")\n",
    "    return df[\"resource.applied_rule\"].unique()\n",
    "\n",
    "def get_user_activity(df,user):\n",
    "    logging.debug(\"getting all records for a given user.\")\n",
    "    return df.loc[df[\"user.email\"] == user]\n",
    "\n",
    "# Polars alternative function\n",
    "def get_user_activity2(df,user):\n",
    "    logging.debug(\"getting all records for a given user.\")\n",
    "    return df.filter(pl.col(\"user.email\") == str(user))\n",
    "\n",
    "def get_user_client_ips(df,user):\n",
    "    logging.debug(\"getting all records for a given user.\")\n",
    "    df1 = df.loc[df[\"user.email\"] == user]\n",
    "    return df1[\"connection.client_ip\"].unique()\n",
    "\n",
    "# Polars alternative function\n",
    "def get_user_client_ips2(df,user):\n",
    "    logging.debug(\"getting all records for a given user.\")\n",
    "    df1 = df.filter(pl.col(\"user.email\") == str(user))\n",
    "    return df1[\"connection.client_ip\"].unique()\n",
    "\n",
    "def get_connector_ids(df):\n",
    "    return df[\"connector.id\"].unique()\n",
    "\n",
    "def get_connector_names(df):\n",
    "    return df[\"connector.name\"].unique()\n",
    "\n",
    "def get_rn_ids(df):\n",
    "    return df[\"remote_network.id\"].unique()\n",
    "\n",
    "def get_rn_names(df):\n",
    "    return df[\"remote_network.name\"].unique()\n",
    "\n",
    "def get_activities_for_remote_network(df,rnid):\n",
    "    #s,dbid = convert_api_id_to_id(rnid)\n",
    "    return df.loc[df[\"remote_network.id\"] == rnid]\n",
    "\n",
    "def get_activities_for_connector(df,connid):\n",
    "    #s,dbid = convert_api_id_to_id(connid)\n",
    "    return df.loc[df[\"connector.id\"] == connid]\n",
    "\n",
    "def get_activity_between_dates(df,before,after,localtz):\n",
    "    # assuming before and after dates are in localtz, they need to be adjusted to UTC before comparison\n",
    "    logging.debug(\"converting before date from string to datetime.\")\n",
    "    dt_before = datetime.datetime.strptime(before, \"%Y-%m-%d %H:%M:%S.%f\")\n",
    "\n",
    "    logging.debug(\"converting after date from string to datetime.\")\n",
    "    dt_after = datetime.datetime.strptime(after, \"%Y-%m-%d %H:%M:%S.%f\")\n",
    "\n",
    "    logging.debug(\"converting before date to UTC.\")\n",
    "    dt_before_utc = tz_to_utc(dt_before,localtz)\n",
    "\n",
    "    logging.debug(\"converting after date to UTC.\")\n",
    "    dt_after_utc = tz_to_utc(dt_after,localtz)\n",
    "\n",
    "    logging.debug(\"before date in UTC: \"+str(dt_before_utc))\n",
    "    logging.debug(\"after date in UTC: \"+str(dt_after_utc))\n",
    "\n",
    "    logging.debug(\"converting UTC dates to epoch.\")\n",
    "    before_epoch = date_to_epoch(str(dt_before_utc).replace(\"+00:00\",\"\"))\n",
    "    logging.debug(\"before date in UTC to epoch: \"+str(before_epoch))\n",
    "\n",
    "    after_epoch = date_to_epoch(str(dt_after_utc).replace(\"+00:00\",\"\"))\n",
    "    logging.debug(\"after date in UTC to epoch: \"+str(after_epoch))\n",
    "\n",
    "    #print(\"before:\"+str(before_epoch)+\" and after: \"+str(after_epoch))\n",
    "    return df[df['timestamp'].between(after_epoch, before_epoch)]\n",
    "\n",
    "# return rows that have an actual error message\n",
    "def get_errors(df):\n",
    "    #return df[~df['connection.error_message'].isna()]\n",
    "    return df[~df['connection.error_message'].str.startswith('NORMAL')]\n",
    "\n",
    "def get_failures(df):\n",
    "    if \"event_type\" in df:\n",
    "        return df.loc[df[\"event_type\"] == \"failed_to_connect\"]\n",
    "\n",
    "# Polars alternative function\n",
    "def get_errors2(df):\n",
    "    return df.filter(pl.col(\"connection.error_message\") != \"NORMAL\")\n",
    "\n",
    "# Polars alternative function\n",
    "def get_failures2(df):\n",
    "    if \"event_type\" in df:\n",
    "        return df.filter(pl.col(\"event_type\") == \"failed_to_connect\")\n",
    "\n",
    "def get_top_addresses(df):\n",
    "    unique_addr_in_df = df[\"resource.address\"].unique()\n",
    "    print(\"address,count,rx,tx\")\n",
    "    for addr in unique_addr_in_df:\n",
    "\n",
    "        total_for_addr = df.loc[df[\"resource.address\"] == addr]\n",
    "        Total_rx = total_for_addr['connection.rx'].sum()\n",
    "        Total_tx = total_for_addr['connection.tx'].sum()\n",
    "\n",
    "        print(str(addr)+\",\"+str(len(total_for_addr))+\",\"+str(Total_rx)+\",\"+str(Total_tx))\n",
    "\n",
    "def get_top_users(df):\n",
    "    unique_users = df[\"user.email\"].unique()\n",
    "\n",
    "    print(\"user,count,rx,tx\")\n",
    "    for user in unique_users:\n",
    "\n",
    "        total_for_user = df.loc[df[\"user.email\"] == user]\n",
    "        Total_rx = total_for_user['connection.rx'].sum()\n",
    "        Total_tx = total_for_user['connection.tx'].sum()\n",
    "\n",
    "        print(str(user)+\",\"+str(len(total_for_user))+\",\"+str(Total_rx)+\",\"+str(Total_tx))\n",
    "\n",
    "\n",
    "################################################################\n",
    "\n",
    "# Ignore the following functions, they provide tooling for other useful functions\n",
    "\n",
    "################################################################\n",
    "\n",
    "# Check if a particular address matches a particular resource definition.\n",
    "# returns True / False, matching string\n",
    "def does_addr_match_res_definition(addr,resource_definition):\n",
    "    regex_definition = resource_definition.replace(\".\",\"\\.\").replace(\"*\",\".*\").replace(\"?\",\".?\")\n",
    "    x = re.search(regex_definition, addr)\n",
    "    if x:\n",
    "        return True,x[0]\n",
    "    else:\n",
    "        return False,\"\"\n",
    "\n",
    "# alternate method to extract IP Geo location info\n",
    "def get_ip_info(addr=''):\n",
    "    str_addr = str(addr)\n",
    "    if str_addr == 'nan':\n",
    "        return {}\n",
    "    else:\n",
    "        url = 'https://ipinfo.io/' + str_addr + '/json'\n",
    "    res = urlopen(url)\n",
    "    #response from url(if res==None then check connection)\n",
    "    data = load(res)\n",
    "    #will load the json response into data\n",
    "    return data\n",
    "\n",
    "# Events in the log show internal DB IDs as opposed to API IDs (visible in the Admin Console)\n",
    "# the following function converts the DB ID to API ID\n",
    "def convert_id_to_api_id(id,objecttype):\n",
    "    if objecttype:\n",
    "        encoded = base64.b64encode((objecttype.capitalize()+\":\"+str(id)).encode('ascii'))\n",
    "        return encoded.decode(\"utf-8\")\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "# Opposite of previous function, converts an API ID to DB ID\n",
    "def convert_api_id_to_id(id):\n",
    "    decoded = base64.b64decode(id)\n",
    "    objname,dbid = decoded.decode(\"utf-8\").split(\":\")\n",
    "    return objname.lower(),dbid\n",
    "\n",
    "# converts an epoch time to a human readable date\n",
    "def epoch_to_date(timestamp,tz):\n",
    "    s, ms = divmod(timestamp, 1000)  # (1236472051, 807)\n",
    "    mydate = '%s.%03d' % (time.strftime('%Y-%m-%d %H:%M:%S', time.gmtime(s)), ms)\n",
    "    utc_time = datetime.datetime.strptime(mydate, \"%Y-%m-%d %H:%M:%S.%f\")\n",
    "    if tz != None:\n",
    "        #logging.debug(\"tz passed as a parameter: \"+tz)\n",
    "        return str(utc_to_tz(utc_time,tz))\n",
    "    else:\n",
    "        #logging.debug(\"tz not passed as a parameter\")\n",
    "        return str(utc_time)\n",
    "\n",
    "def utc_to_tz(utc_time,tz):\n",
    "    utc = ZoneInfo('UTC')\n",
    "    localtz = ZoneInfo(tz)\n",
    "    utctime = utc_time.replace(tzinfo=utc)\n",
    "    localtime = utctime.astimezone(localtz)\n",
    "    return localtime\n",
    "\n",
    "def tz_to_utc(a_time,tz):\n",
    "    utc = ZoneInfo('UTC')\n",
    "    localtz = ZoneInfo(tz)\n",
    "    atime = a_time.replace(tzinfo=localtz)\n",
    "    utc_time = atime.astimezone(utc)\n",
    "    return utc_time\n",
    "\n",
    "# converts a human readable date to epoch time\n",
    "def date_to_epoch(adate):\n",
    "    utc_time = datetime.datetime.strptime(adate, \"%Y-%m-%d %H:%M:%S.%f\")\n",
    "    epoch_time = (utc_time - datetime.datetime(1970, 1, 1)).total_seconds()\n",
    "    return epoch_time*1000\n",
    "\n",
    "# returns a DF as CSV\n",
    "def get_df_as_csv(df):\n",
    "    return df.to_csv(index=True)\n",
    "\n",
    "def initiate_logging():\n",
    "    root = logging.getLogger()\n",
    "    root.setLevel(logging.INFO)\n",
    "    handler = logging.StreamHandler(sys.stdout)\n",
    "    handler.setLevel(logging.INFO)\n",
    "    formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "    handler.setFormatter(formatter)\n",
    "    root.addHandler(handler)\n",
    "\n",
    "def get_state(ip):\n",
    "    if ip in ip_info_dict and str(ip) != \"nan\":\n",
    "        data = ip_info_dict[ip]\n",
    "        if 'message' in data:\n",
    "            if 'bogon' in data['message']:\n",
    "                return(\"private\")\n",
    "            #else:\n",
    "            #    print(data['message'])\n",
    "        else:\n",
    "            return(data['state_code'])\n",
    "    else:\n",
    "        return(\"IP not in dict: \"+str(ip))\n",
    "\n",
    "def get_country(ip):\n",
    "    if ip in ip_info_dict and str(ip) != \"nan\":\n",
    "        data = ip_info_dict[ip]\n",
    "        if 'message' in data:\n",
    "            if 'bogon' in data['message']:\n",
    "                return(\"private\")\n",
    "            #else:\n",
    "            #    print(data['message'])\n",
    "        else:\n",
    "            return(data['country_code3'])\n",
    "    else:\n",
    "        return(\"IP not in dict: \"+str(ip))\n",
    "\n",
    "def get_city(ip):\n",
    "    if ip in ip_info_dict and str(ip) != \"nan\":\n",
    "        data = ip_info_dict[ip]\n",
    "        if 'message' in data:\n",
    "            if 'bogon' in data['message']:\n",
    "                return(\"private\")\n",
    "            #else:\n",
    "            #    print(data['message'])\n",
    "        else:\n",
    "            return(data['city'])\n",
    "    else:\n",
    "        return(\"IP not in dict: \"+str(ip))\n",
    "\n",
    "# initiate logging (no need to change this)\n",
    "initiate_logging()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-fcVwLH2P41t"
   },
   "source": [
    "# Generate the final (XLSX) Report\n",
    "\n",
    "You can customize the output report below in many ways:\n",
    "\n",
    "* Change the title of each Tab\n",
    "* Activate or deactivate sections of the report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "C9VBI_V4Qbt4",
    "outputId": "2982d98e-d17b-48f7-a19e-2e2c9ea5585e",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting imported report to DataFrame\n",
      "Report has been successfully converted.\n",
      "extracting User Public IPs\n",
      "extracting Error Report\n",
      "extracting Resource Matching Report\n",
      "extracting DNS Errors\n",
      "extracting Connection Errors\n",
      "extracting Overall Connector Stats\n",
      "extracting Per Connector Report\n",
      "writing IP information...\n",
      "writing activity summary...\n",
      "writing resource summary...\n",
      "writing resource match summary...\n",
      "wrinting DNS error summary...\n",
      "wrinting Connection error summary...\n",
      "writing connector summary...\n",
      "writing per Connector stats summary...\n",
      "Report Generation done. Here is how long it took to generate: 98662.09888458252ms\n"
     ]
    }
   ],
   "source": [
    "import polars as pl\n",
    "import time\n",
    "import json,requests,re\n",
    "from types import SimpleNamespace\n",
    "import datetime, time, logging, sys, base64, math\n",
    "from zoneinfo import ZoneInfo\n",
    "from urllib.request import urlopen\n",
    "from json import load\n",
    "from xlsxwriter import Workbook\n",
    "\n",
    "doUserIPInfo = True # Export User/IP Info\n",
    "userIPSummaryTitle = \"User IP Details\" # Title for Workbook Tab\n",
    "\n",
    "doUserActInfo = True # Export User Activity Details\n",
    "userActTitle = \"User Activity Details\" # Title for Workbook Tab\n",
    "\n",
    "doResInfo = True # Export Full Resource List\n",
    "doResTitle = \"Full Resource List\" # Title for Workbook Tab\n",
    "\n",
    "doMatchList = True # Export Match List for Wildcard Resources\n",
    "doMatchTitle = \"Resource Matching List\" # Title for Workbook Tab\n",
    "\n",
    "doConnectorsStats = True # Export Overall Connectors Stats\n",
    "doConnectorsStatsTitle = \"Connector Activities\" # Title for Workbook Tab\n",
    "\n",
    "doConnectorsStatsPerConnector = True # Export Per Connectors Stats\n",
    "\n",
    "doConnErrorsStats = True # Export Connection Error Stats\n",
    "doConnErrorsStatsTitle = \"Connection Errors\" # Title for Workbook Tab\n",
    "\n",
    "doDNSErrorsStats = True # Export DNS Error Stats\n",
    "doDNSErrorsStatsTitle = \"DNS Errors\" # Title for Workbook Tab\n",
    "\n",
    "# initiate logging (no need to change this)\n",
    "initiate_logging()\n",
    "startTime = time.time()\n",
    "\n",
    "# Convert entire report output to DataFrame for processing.\n",
    "#local_tz = 'America/Los_Angeles'\n",
    "print(\"Converting imported report to DataFrame\")\n",
    "df = convert_admin_console_report_to_df(full_console_report,local_tz)\n",
    "print(\"Report has been successfully converted.\")\n",
    "\n",
    "if (doUserIPInfo or doUserActInfo):\n",
    "    unique_users = df[\"user.email\"].unique()\n",
    "\n",
    "# End User Public IP tab\n",
    "if (doUserIPInfo):\n",
    "### Get User IP Info\n",
    "    print(\"extracting User Public IPs\")\n",
    "\n",
    "    user_ip_info_data = {\n",
    "        \"user\":[],\n",
    "        \"ipinfo\":[],\n",
    "    }\n",
    "\n",
    "    user_ip_info_df = pl.DataFrame(user_ip_info_data, schema={\"user\":pl.String, \"ipinfo\":pl.String})\n",
    "\n",
    "    for user in unique_users:\n",
    "        ips = get_user_client_ips2(df,user)\n",
    "        iplist_with_geo = []\n",
    "        for ip in ips:\n",
    "            iplist_with_geo.append(str(ip))\n",
    "            ip_info_for_user = \",\".join(iplist_with_geo)\n",
    "\n",
    "        temp_ip_summary_data = { \"user\": user, \"ipinfo\":ip_info_for_user }\n",
    "        temp_ip_summary_df = pl.DataFrame(temp_ip_summary_data, schema={\"user\":pl.String, \"ipinfo\":pl.String})\n",
    "        user_ip_info_df = pl.concat([user_ip_info_df,temp_ip_summary_df])\n",
    "\n",
    "    user_ip_info_df = user_ip_info_df.rename({\"user\":\"User Email\",\"ipinfo\" : \"Public IPs\"})\n",
    "\n",
    "# End User activity tab\n",
    "if (doUserActInfo):\n",
    "    user_activity_data = {\n",
    "        \"user\":[],\n",
    "        \"numconn\":[],\n",
    "        \"numerr\":[],\n",
    "        \"txbytes\":[],\n",
    "        \"rxbytes\":[]\n",
    "    }\n",
    "    user_activity_df = pl.DataFrame(user_activity_data, schema={\"user\":pl.String, \"numconn\":pl.Int32, \"numerr\":pl.Int32, \"txbytes\":pl.Int64, \"rxbytes\": pl.Int64})\n",
    "    for user in unique_users:\n",
    "        user_act = get_user_activity2(df,user)\n",
    "        user_err = get_errors2(user_act)\n",
    "        total_tx = user_act['connection.tx'].sum()\n",
    "        total_rx = user_act['connection.rx'].sum()\n",
    "        temp_act_data = { \"user\": user, \"numconn\":len(user_act), \"numerr\": len(user_err), \"txbytes\":total_tx, \"rxbytes\":total_rx }\n",
    "        temp_act_df = pl.DataFrame(temp_act_data, schema={\"user\":pl.String, \"numconn\":pl.Int32, \"numerr\":pl.Int32, \"txbytes\":pl.Int64, \"rxbytes\": pl.Int64})\n",
    "        user_activity_df = pl.concat([user_activity_df, temp_act_df])\n",
    "\n",
    "    user_activity_df = user_activity_df.rename({\"user\":\"User Email\",\"numconn\" : \"Nb of Connections\",\"numerr\" : \"Nb of Errors\",\"txbytes\" : \"TX (bytes)\",\"rxbytes\":\"RX (bytes)\"})\n",
    "\n",
    "# Overall Resource performance tab\n",
    "if (doResInfo):\n",
    "    all_errors = get_errors2(df)\n",
    "    unique_addr_in_df = df[\"resource.address\"].unique()\n",
    "    res_data = {\n",
    "        \"address\":[],\n",
    "        \"resourcedef\":[], #Feb 3 2025\n",
    "        \"resourceids\":[], #Feb 3 2025\n",
    "        \"rnetworks\":[], #Feb 3 2025\n",
    "        \"numconn\":[],\n",
    "        \"numerrs\":[],\n",
    "        \"failrate\":[],\n",
    "        \"totaltx\":[],\n",
    "        \"totalrx\":[],\n",
    "        \"tcp\":[],\n",
    "        \"udp\":[],\n",
    "        \"icmp\":[]\n",
    "    }\n",
    "    res_data_df = pl.DataFrame(res_data, schema={\"address\":pl.String, \"resourcedef\":pl.String, \"resourceids\":pl.String, \"rnetworks\":pl.String, \"numconn\":pl.Int32, \"numerrs\":pl.Int32, \"failrate\":pl.Float32, \"totaltx\":pl.Int64, \"totalrx\":pl.Int64, \"tcp\":pl.String, \"udp\":pl.String, \"icmp\":pl.String})\n",
    "    \n",
    "    print(\"extracting Error Report\")\n",
    "    # print(\"address,number of connections,number of errors,failure rate(%),total tx, total rx,TCP[Ports],UDP[Ports],ICMP[Ports]\")\n",
    "    # ## for each address in the error list, check how many records are in error and what the percentage of errors is\n",
    "    for addr in unique_addr_in_df:\n",
    "        if str(addr) != \"nan\":\n",
    "            total_for_addr = df.filter(pl.col(\"resource.address\") == addr)\n",
    "            err_for_addr = all_errors.filter(pl.col(\"resource.address\") == addr)\n",
    "\n",
    "            if len(total_for_addr) > 0:\n",
    "                percentage_failure = round(len(err_for_addr) / len(total_for_addr) * 100,2)\n",
    "            else:\n",
    "                percentage_failure = 100\n",
    "\n",
    "            port_connected = []\n",
    "            addr_activities = df.filter(pl.col(\"resource.address\") == addr)\n",
    "            \n",
    "            # \"resource_id\" : \"resource.id\",\"remote_network\" : \"remote_network.name\",\"remote_network_id\" : \"remote_network.id\"\n",
    "    \n",
    "            # feb 3 2025 - resource definitions\n",
    "            res_definitions = addr_activities[\"resource.applied_rule\"].unique()\n",
    "            res_ids = addr_activities[\"resource.id\"].unique()\n",
    "            rnetwork_names = addr_activities[\"remote_network.name\"].unique()\n",
    "\n",
    "            res_def_list=[]\n",
    "            for definition in res_definitions:\n",
    "                res_def_list.append(definition)\n",
    "            res_def_list_s=\";\".join(str(x) for x in res_def_list)\n",
    "            \n",
    "            res_id_list=[]\n",
    "            for definition in res_ids:\n",
    "                res_id_list.append(definition)\n",
    "            res_id_list_s=\";\".join(str(x) for x in res_id_list)\n",
    "            \n",
    "            res_rnetwork_list=[]\n",
    "            for definition in rnetwork_names:\n",
    "                res_rnetwork_list.append(definition)\n",
    "            res_rnetwork_list_s=\";\".join(str(x) for x in res_rnetwork_list)\n",
    "            # Feb 3 2025 end\n",
    "            \n",
    "            total_tx = addr_activities['connection.tx'].sum()\n",
    "            total_rx = addr_activities['connection.rx'].sum()\n",
    "\n",
    "            addr_activities_protocols = addr_activities[\"connection.protocol\"].unique()\n",
    "            addr_activities_icmp = addr_activities.filter(pl.col(\"connection.protocol\") == \"icmp\")\n",
    "            addr_activities_tcp = addr_activities.filter(pl.col(\"connection.protocol\") == \"tcp\")\n",
    "            addr_activities_udp = addr_activities.filter(pl.col(\"connection.protocol\") == \"udp\")\n",
    "\n",
    "            ports_tcp = addr_activities_tcp[\"connection.resource_port\"].unique()\n",
    "            ports_udp = addr_activities_udp[\"connection.resource_port\"].unique()\n",
    "            ports_icmp = addr_activities_icmp[\"connection.resource_port\"].unique()\n",
    "\n",
    "            tcp_port_list = []\n",
    "            udp_port_list = []\n",
    "            icmp_port_list = []\n",
    "            for port in ports_tcp:\n",
    "                tcp_port_list.append(port)\n",
    "            for port in ports_udp:\n",
    "                udp_port_list.append(port)\n",
    "            for port in ports_icmp:\n",
    "                icmp_port_list.append(port)\n",
    "            tcp_port_list_s=\";\".join(str(x) for x in tcp_port_list)\n",
    "            udp_port_list_s=\";\".join(str(x) for x in udp_port_list)\n",
    "            icmp_port_list_s=\";\".join(str(x) for x in icmp_port_list)\n",
    "            \n",
    "            res_data_temp_df = pl.DataFrame(res_data, schema={\"address\":pl.String, \"resourcedef\":pl.String, \"resourceids\":pl.String, \"rnetworks\":pl.String, \"numconn\":pl.Int32, \"numerrs\":pl.Int32, \"failrate\":pl.Float32, \"totaltx\":pl.Int64, \"totalrx\":pl.Int64, \"tcp\":pl.String, \"udp\":pl.String, \"icmp\":pl.String})\n",
    "\n",
    "\n",
    "            temp_res_data = {\"address\":str(addr),\"resourcedef\":str(res_def_list_s),\"resourceids\":str(res_id_list_s),\"rnetworks\":str(res_rnetwork_list_s), \"numconn\":len(total_for_addr), \"numerrs\":len(err_for_addr), \"failrate\":percentage_failure, \"totaltx\":total_tx, \"totalrx\":total_rx, \"tcp\":tcp_port_list_s, \"udp\":udp_port_list_s, \"icmp\":icmp_port_list_s}\n",
    "            temp_res_df = pl.DataFrame(temp_res_data, schema={\"address\":pl.String, \"resourcedef\":pl.String, \"resourceids\":pl.String, \"rnetworks\":pl.String, \"numconn\":pl.Int32, \"numerrs\":pl.Int32, \"failrate\":pl.Float32, \"totaltx\":pl.Int64, \"totalrx\":pl.Int64, \"tcp\":pl.String, \"udp\":pl.String, \"icmp\":pl.String})\n",
    "            res_data_df = pl.concat([res_data_df, temp_res_df])\n",
    "\n",
    "    res_data_df = res_data_df.rename({\"address\":\"Resource Address\",\"resourcedef\":\"Resource Definitions\",\"resourceids\":\"Resource IDs\",\"rnetworks\":\"Remote Networks\",\"numconn\" : \"Nb of Connections\",\"numerrs\" : \"Nb of Errors\",\"totaltx\" : \"TX (bytes)\",\"totalrx\":\"RX (bytes)\",\"failrate\":\"Failure Rate (%)\"})\n",
    "\n",
    "# Resource matching list\n",
    "if (doMatchList):\n",
    "    # get the list of unique resource definitions that have caught traffic (with the list of actual addresses hit for each resource)\n",
    "    print(\"extracting Resource Matching Report\")\n",
    "    unique_resources = df[\"resource.applied_rule\"].unique()\n",
    "    match_data = {\n",
    "        \"resource\":[],\n",
    "        \"matches\":[]\n",
    "    }\n",
    "    match_data_df = pl.DataFrame(match_data, schema={\"resource\":pl.String, \"matches\":pl.String})\n",
    "    for r in unique_resources:\n",
    "        addresses_for_resource,ips_for_resource = get_endpoints_from_resource_activity2(df,r)\n",
    "        \n",
    "        # if the list of unique addresses returns only 1 address and said address is None, then the source of truth should be IPs and not FQDNs\n",
    "        if len(addresses_for_resource) == 1:\n",
    "            addr_joined=\"; \".join(str(x) for x in addresses_for_resource)\n",
    "            if addr_joined == \"None\":\n",
    "                addr_joined=\"; \".join(str(x) for x in ips_for_resource)\n",
    "        else:\n",
    "            addr_joined=\"; \".join(str(x) for x in addresses_for_resource) \n",
    "                \n",
    "        \n",
    "        #if(len(addresses_for_resource) != 0):\n",
    "        #    addr_joined=\"; \".join(str(x) for x in addresses_for_resource)\n",
    "        #else:\n",
    "        #    addr_joined = \"N/A\"\n",
    "        temp_match_data = {\"resource\":r, \"matches\":addr_joined}\n",
    "        temp_match_data_df = pl.DataFrame(temp_match_data, schema={\"resource\":pl.String, \"matches\":pl.String})\n",
    "        match_data_df = pl.concat([match_data_df, temp_match_data_df])\n",
    "\n",
    "    match_data_df = match_data_df.rename({\"resource\":\"Resource Definition\",\"matches\" : \"Corresponding Addresses\"})\n",
    "\n",
    "# DNS Error tab\n",
    "if(doDNSErrorsStats):\n",
    "    print(\"extracting DNS Errors\")\n",
    "\n",
    "    connact_data = {\n",
    "            \"address\":[],\n",
    "            \"nberrors\":[]\n",
    "      }\n",
    "\n",
    "    dns_errors_df = pl.DataFrame(connact_data, schema={\"address\":pl.String, \"nberrors\":pl.Int64})\n",
    "    activities_dnserror = df.filter(pl.col(\"connection.error_message\") == \"DNS_ERROR\")\n",
    "    unique_dns_failures = activities_dnserror['resource.address'].unique()\n",
    "\n",
    "    for addr in unique_dns_failures:\n",
    "        dns_ct = activities_dnserror.filter(pl.col(\"resource.address\") == addr).select(pl.len()).item()\n",
    "\n",
    "        temp_dns_errors = {\"address\":addr, \"nberrors\":dns_ct}\n",
    "        temp_dns_errors_df = pl.DataFrame(temp_dns_errors, schema={\"address\":pl.String, \"nberrors\":pl.Int64})\n",
    "\n",
    "        dns_errors_df = pl.concat([dns_errors_df, temp_dns_errors_df])\n",
    "\n",
    "    dns_errors_df = dns_errors_df.rename({\"address\":\"Resource Address\",\"nberrors\" : \"Number of DNS Errors\"})\n",
    "\n",
    "# Connection Error tab\n",
    "if(doConnErrorsStats):\n",
    "    print(\"extracting Connection Errors\")\n",
    "\n",
    "    connact_data = {\n",
    "        \"address\":[],\n",
    "        \"nberrors\":[]\n",
    "    }\n",
    "\n",
    "    conn_errors_df = pl.DataFrame(connact_data, schema={\"address\":pl.String, \"nberrors\":pl.Int64})\n",
    "    activities_connectionfailed = df.filter(pl.col(\"connection.error_message\") == \"CONNECTION_FAILED\")\n",
    "    unique_address_failures = activities_connectionfailed['resource.address'].unique()\n",
    "\n",
    "    for addr in unique_address_failures:\n",
    "        err_ct = activities_connectionfailed.filter(pl.col(\"resource.address\") == addr).select(pl.len()).item()\n",
    "\n",
    "        temp_conn_errors = {\"address\":addr, \"nberrors\":err_ct}\n",
    "        temp_conn_errors_df = pl.DataFrame(temp_conn_errors, schema={\"address\":pl.String, \"nberrors\":pl.Int64})\n",
    "\n",
    "        conn_errors_df = pl.concat([conn_errors_df, temp_conn_errors_df])\n",
    "\n",
    "    conn_errors_df = conn_errors_df.rename({\"address\":\"Resource Address\",\"nberrors\" : \"Number of Connection Errors\"})\n",
    "\n",
    "# Overall Connector performance tab\n",
    "if (doConnectorsStats):\n",
    "    print(\"extracting Overall Connector Stats\")\n",
    "    unique_connectors = df[\"connector.name\"].unique()\n",
    "    connact_data = {\n",
    "        \"connectorname\":[],\n",
    "        \"rnname\":[],\n",
    "        \"totalactivities\":[],\n",
    "        \"connsuccess\":[],\n",
    "        \"connerror\":[],\n",
    "        \"dnserror\":[]\n",
    "    }\n",
    "    connector_data_df = pl.DataFrame(connact_data, schema={\"connectorname\":pl.String,\"rnname\":pl.String, \"totalactivities\":pl.Int64, \"connsuccess\":pl.Int64, \"connerror\":pl.Int64, \"dnserror\":pl.Int64})\n",
    "    for connector in unique_connectors:\n",
    "\n",
    "        conn_activities = df.filter(pl.col(\"connector.name\") == connector)\n",
    "        rn_name = conn_activities['remote_network.name'].unique()[0]\n",
    "        conn_activities_normal = conn_activities.filter(pl.col(\"connection.error_message\") == \"NORMAL\")\n",
    "        conn_activities_connectionfailed = conn_activities.filter(pl.col(\"connection.error_message\") == \"CONNECTION_FAILED\")\n",
    "        conn_activities_dnserror = conn_activities.filter(pl.col(\"connection.error_message\") == \"DNS_ERROR\")\n",
    "\n",
    "        temp_res_data = {\"connectorname\":str(connector),\"rnname\":str(rn_name), \"totalactivities\":len(conn_activities),\"connsuccess\":len(conn_activities_normal),\"connerror\":len(conn_activities_connectionfailed),\"dnserror\":len(conn_activities_dnserror)}\n",
    "        temp_res_df = pl.DataFrame(temp_res_data, schema={\"connectorname\":pl.String,\"rnname\":pl.String, \"totalactivities\":pl.Int64, \"connsuccess\":pl.Int64, \"connerror\":pl.Int64, \"dnserror\":pl.Int64})\n",
    "\n",
    "        connector_data_df = pl.concat([connector_data_df, temp_res_df])\n",
    "\n",
    "    connector_data_df = connector_data_df.rename({\"connectorname\":\"Connector Name\",\"totalactivities\" : \"Nb of Activities\",\"rnname\":\"Remote Network Name\",\"connsuccess\":\"Successful Connections\",\"connerror\":\"Failed Connections\",\"dnserror\":\"DNS Errors\"})\n",
    "\n",
    "# Per Connector performance tab\n",
    "if (doConnectorsStatsPerConnector):\n",
    "    print(\"extracting Per Connector Report\")\n",
    "    unique_connectors = df[\"connector.name\"].unique()\n",
    "    connact_data = {\n",
    "        \"date\":[],\n",
    "        \"connectorname\":[],\n",
    "        \"rnname\":[],\n",
    "        \"totalactivities\":[],\n",
    "        \"connsuccess\":[],\n",
    "        \"connerror\":[],\n",
    "        \"dnserror\":[]\n",
    "    }\n",
    "\n",
    "    all_connector_data = {}\n",
    "    for connector in unique_connectors:\n",
    "        per_connector_data_df = pl.DataFrame(connact_data, schema={\"date\":pl.String,\"connectorname\":pl.String,\"rnname\":pl.String, \"totalactivities\":pl.Int64, \"connsuccess\":pl.Int64, \"connerror\":pl.Int64, \"dnserror\":pl.Int64})\n",
    "        conn_activities = df.filter(pl.col(\"connector.name\") == connector)\n",
    "        rn_name = conn_activities['remote_network.name'].unique()[0]\n",
    "        unique_days_of_Activities = conn_activities[\"timestamp.yymmdd\"].unique()\n",
    "        for day in unique_days_of_Activities:\n",
    "            conn_day_activities = conn_activities.filter(pl.col(\"timestamp.yymmdd\") == day)\n",
    "            connection_normal = conn_day_activities.filter(pl.col(\"connection.error_message\") == \"NORMAL\")\n",
    "            connection_errors = conn_day_activities.filter(pl.col(\"connection.error_message\") == \"CONNECTION_FAILED\")\n",
    "            dns_errors = conn_day_activities.filter(pl.col(\"connection.error_message\") == \"DNS_ERROR\")\n",
    "\n",
    "            temp_res_data = {\"date\":str(day),\"connectorname\":str(connector),\"rnname\":str(rn_name), \"totalactivities\":len(conn_day_activities),\"connsuccess\":len(connection_normal),\"connerror\":len(connection_errors),\"dnserror\":len(dns_errors)}\n",
    "\n",
    "            temp_res_df = pl.DataFrame(temp_res_data, schema={\"date\":pl.String,\"connectorname\":pl.String,\"rnname\":pl.String, \"totalactivities\":pl.Int64, \"connsuccess\":pl.Int64, \"connerror\":pl.Int64, \"dnserror\":pl.Int64})\n",
    "\n",
    "            per_connector_data_df = pl.concat([per_connector_data_df, temp_res_df])\n",
    "\n",
    "        per_connector_data_df = per_connector_data_df.rename({\"connectorname\":\"Connector Name\",\"totalactivities\" : \"Nb of Activities\",\"rnname\":\"Remote Network Name\",\"connsuccess\":\"Successful Connections\",\"connerror\":\"Failed Connections\",\"dnserror\":\"DNS Errors\"})\n",
    "\n",
    "        all_connector_data[connector] = per_connector_data_df\n",
    "\n",
    "# Generate the XLSX Report with all tabs & data\n",
    "with Workbook(workbookName) as wb:\n",
    "\n",
    "    if(doUserIPInfo):\n",
    "        print(\"writing IP information...\")\n",
    "        user_ip_info_df.write_excel(\n",
    "            workbook=wb,\n",
    "            worksheet=userIPSummaryTitle\n",
    "        )\n",
    "    if(doUserActInfo):\n",
    "        print(\"writing activity summary...\")\n",
    "        user_activity_df.write_excel(\n",
    "            workbook=wb,\n",
    "            worksheet=userActTitle\n",
    "        )\n",
    "\n",
    "    if(doResInfo):\n",
    "        print(\"writing resource summary...\")\n",
    "        res_data_df.write_excel(\n",
    "            workbook=wb,\n",
    "            worksheet=doResTitle\n",
    "        )\n",
    "    if(doMatchList):\n",
    "        print(\"writing resource match summary...\")\n",
    "        match_data_df.write_excel(\n",
    "            workbook=wb,\n",
    "            worksheet=doMatchTitle\n",
    "        )\n",
    "    if(doDNSErrorsStats):\n",
    "        print(\"wrinting DNS error summary...\")\n",
    "        dns_errors_df.write_excel(\n",
    "            workbook=wb,\n",
    "            worksheet=doDNSErrorsStatsTitle\n",
    "      )\n",
    "\n",
    "    if(doConnErrorsStats):\n",
    "        print(\"wrinting Connection error summary...\")\n",
    "        conn_errors_df.write_excel(\n",
    "            workbook=wb,\n",
    "            worksheet=doConnErrorsStatsTitle\n",
    "      )\n",
    "    if(doConnectorsStats):\n",
    "        print(\"writing connector summary...\")\n",
    "        connector_data_df.write_excel(\n",
    "        workbook=wb,\n",
    "        worksheet=doConnectorsStatsTitle\n",
    "    )\n",
    "\n",
    "    if(doConnectorsStatsPerConnector):\n",
    "        print(\"writing per Connector stats summary...\")\n",
    "        #all_connector_data\n",
    "        for key in all_connector_data:\n",
    "            all_connector_data[key].write_excel(\n",
    "                workbook=wb,\n",
    "                worksheet=key\n",
    "            )\n",
    "\n",
    "    for WS in wb.worksheets():\n",
    "        WS.autofit()\n",
    "        WS.autofilter('A1:Z1000')\n",
    "\n",
    "    endTime = time.time()\n",
    "    timeDiff = endTime - startTime\n",
    "    timeDiff = timeDiff * 10**3\n",
    "    print(\"Report Generation done. Here is how long it took to generate: \" + str(timeDiff) + \"ms\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
