{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e8Dor7X7IHDp"
   },
   "source": [
    "## Intro\n",
    "\n",
    "This notebook generates an xlsx report providing insights into your Twingate environment from Network Events across all Remote Networks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Required Python Dependencies\n",
    "\n",
    "This notebook uses dataframe and polars in order to process large network event datasets and outputs a xlsx spreadsheet, it therefore requires some very specific python libraries to work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nXI-WvVs7uYY",
    "outputId": "cc2b8279-722e-4c5f-ecfd-f5e31abd8e9e",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Cellar/python@3.9/3.9.20/Frameworks/Python.framework/Versions/3.9/lib/python3.9/pty.py:85: RuntimeWarning: Using fork() can cause Polars to deadlock in the child process.\n",
      "In addition, using fork() with Python in general is a recipe for mysterious\n",
      "deadlocks and crashes.\n",
      "\n",
      "The most likely reason you are seeing this error is because you are using the\n",
      "multiprocessing module on Linux, which uses fork() by default. This will be\n",
      "fixed in Python 3.14. Until then, you want to use the \"spawn\" context instead.\n",
      "\n",
      "See https://docs.pola.rs/user-guide/misc/multiprocessing/ for details.\n",
      "\n",
      "If you really know what your doing, you can silence this warning with the warning module\n",
      "or by setting POLARS_ALLOW_FORKING_THREAD=1.\n",
      "\n",
      "  pid, fd = os.forkpty()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mDEPRECATION: Configuring installation scheme with distutils config files is deprecated and will no longer work in the near future. If you are using a Homebrew or Linuxbrew Python, please see discussion at https://github.com/Homebrew/homebrew-core/issues/76621\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: xlsxwriter in /opt/homebrew/lib/python3.9/site-packages (3.2.0)\n",
      "\u001b[33mWARNING: Error parsing dependencies of bleach: Expected matching RIGHT_PARENTHESIS for LEFT_PARENTHESIS, after version specifier\n",
      "    tinycss2 (>=1.1.0<1.2) ; extra == 'css'\n",
      "             ~~~~~~~~^\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mDEPRECATION: Configuring installation scheme with distutils config files is deprecated and will no longer work in the near future. If you are using a Homebrew or Linuxbrew Python, please see discussion at https://github.com/Homebrew/homebrew-core/issues/76621\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3.9 -m pip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "\u001b[33mDEPRECATION: Configuring installation scheme with distutils config files is deprecated and will no longer work in the near future. If you are using a Homebrew or Linuxbrew Python, please see discussion at https://github.com/Homebrew/homebrew-core/issues/76621\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: pandas in /opt/homebrew/lib/python3.9/site-packages (1.4.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /opt/homebrew/lib/python3.9/site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/homebrew/lib/python3.9/site-packages (from pandas) (2022.1)\n",
      "Requirement already satisfied: numpy>=1.20.0 in /opt/homebrew/lib/python3.9/site-packages (from pandas) (1.22.4)\n",
      "Requirement already satisfied: six>=1.5 in /opt/homebrew/lib/python3.9/site-packages (from python-dateutil>=2.8.1->pandas) (1.16.0)\n",
      "\u001b[33mWARNING: Error parsing dependencies of bleach: Expected matching RIGHT_PARENTHESIS for LEFT_PARENTHESIS, after version specifier\n",
      "    tinycss2 (>=1.1.0<1.2) ; extra == 'css'\n",
      "             ~~~~~~~~^\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mDEPRECATION: Configuring installation scheme with distutils config files is deprecated and will no longer work in the near future. If you are using a Homebrew or Linuxbrew Python, please see discussion at https://github.com/Homebrew/homebrew-core/issues/76621\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3.9 -m pip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "\u001b[33mDEPRECATION: Configuring installation scheme with distutils config files is deprecated and will no longer work in the near future. If you are using a Homebrew or Linuxbrew Python, please see discussion at https://github.com/Homebrew/homebrew-core/issues/76621\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: polars in /opt/homebrew/lib/python3.9/site-packages (1.17.1)\n",
      "\u001b[33mWARNING: Error parsing dependencies of bleach: Expected matching RIGHT_PARENTHESIS for LEFT_PARENTHESIS, after version specifier\n",
      "    tinycss2 (>=1.1.0<1.2) ; extra == 'css'\n",
      "             ~~~~~~~~^\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mDEPRECATION: Configuring installation scheme with distutils config files is deprecated and will no longer work in the near future. If you are using a Homebrew or Linuxbrew Python, please see discussion at https://github.com/Homebrew/homebrew-core/issues/76621\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3.9 -m pip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# Install python dependencies required\n",
    "%pip install xlsxwriter\n",
    "%pip install pandas\n",
    "%pip install polars"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Specify the Network Event Report to process\n",
    "\n",
    "Change the cell below and specify the full path to the CSV extracted from your Admin Console along with the output xlsx file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "4N7_mjfH_T0g"
   },
   "outputs": [],
   "source": [
    "# specify the timezone you are in, it will become visible in the dataframe and can be used to filter activities based on localtime\n",
    "local_tz = 'America/Los_Angeles'\n",
    "full_console_report = '/SomePath/admin_console_export_file.csv'\n",
    "workbookName = '/SomePath/insight_report.xlsx'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z-je1RV8JRC2"
   },
   "source": [
    "# Load up the parsing functions below\n",
    "\n",
    "The functions in the following cell are used to generate the final report, they can be combined and used together as well to produce more insights as needed by Admins.\n",
    "\n",
    "The cell below only loads functions in memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "4rp2jWMXz4ql"
   },
   "outputs": [],
   "source": [
    "import json,requests,re\n",
    "import pandas as pd\n",
    "from types import SimpleNamespace\n",
    "import datetime, time, logging, sys, base64, math\n",
    "from urllib.request import urlopen\n",
    "from json import load\n",
    "\n",
    "################################################################\n",
    "\n",
    "# Use the following functions to extract what you need\n",
    "\n",
    "################################################################\n",
    "\n",
    "# converts an admin console connector reports to a flattened normalized DF\n",
    "def convert_admin_console_report_to_df(conn_output,tz=None):\n",
    "    df = pl.read_csv(conn_output)\n",
    "    \n",
    "    df = df.rename({\"device_id\":\"device.id\",\"start_time\" : \"timestamp.readable\",\"status\" : \"connection.error_message\",\"resource_domain\" : \"resource.address\",\"applied_rule\" : \"resource.applied_rule\",\"bytes_received\" : \"connection.rx\",\"bytes_transferred\" : \"connection.tx\",\"resource_id\" : \"resource.id\",\"remote_network\" : \"remote_network.name\",\"remote_network_id\" : \"remote_network.id\",\"protocol\" : \"connection.protocol\",\"resource_port\" : \"connection.resource_port\",\"resource_ip\" : \"connection.resource_ip\",\"connector_id\" : \"connector.id\",\"user\": \"user.email\", \"user_id\": \"user.id\", \"client_ip\" : \"connection.client_ip\",\"connector\" : \"connector.name\"})\n",
    "    #df = df.drop(['relays', 'relay_ips','relay_ports','end_time'])\n",
    "    df = df.rename({\"relay_ips\":\"relay.ips\",\"relay_ports\" : \"relay.ports\"})\n",
    "    df = df.drop(['end_time'])\n",
    "    df = df.with_columns(pl.col(\"timestamp.readable\").str.replace(r\" UTC\", \"\"))\n",
    "    df = df.with_columns(pl.col('timestamp.readable').str.strptime(pl.Datetime, \"%Y-%m-%d %H:%M:%S%.f\").dt.convert_time_zone(tz))\n",
    "    df = df.with_columns(pl.format(\"ConnectorId:{}\", \"connector.id\").str.encode(\"base64\").name.keep())\n",
    "    df = df.with_columns(pl.format(\"RemoteNetworkId:{}\", \"remote_network.id\").str.encode(\"base64\").name.keep())\n",
    "    df = df.with_columns(pl.format(\"ResourceId:{}\", \"resource.id\").str.encode(\"base64\").name.keep())\n",
    "    df = df.with_columns(pl.format(\"UserId:{}\", \"user.id\").str.encode(\"base64\").name.keep())\n",
    "    df = df.with_columns(pl.format(\"DeviceId:{}\", \"device.id\").str.encode(\"base64\").name.keep())\n",
    "    df = df.with_columns(pl.col(\"connection.error_message\")).fill_nan('NORMAL')\n",
    "    df = df.with_columns((pl.col(\"timestamp.readable\").dt.strftime('%Y-%m-%d')).alias(\"timestamp.yymmdd\"))\n",
    "    # Jan 31 2024 - resource.address is empty when the resource definition is a cidr block and instead resource_ip os used\n",
    "    # we replace empty values in resource.address with the ip instead\n",
    "    df = df.with_columns(\n",
    "        pl.col(\"resource.address\").fill_null(pl.col(\"connection.resource_ip\"))\n",
    "    )\n",
    "\n",
    "    return df\n",
    "\n",
    "# Polars alternative function\n",
    "def get_endpoints_from_resource_activity2(df,resource):\n",
    "    logging.debug(\"getting list of activities for a given resource.\")\n",
    "    adf = df.filter(pl.col(\"resource.applied_rule\") == resource)\n",
    "    ResAddresses = adf[\"resource.address\"].unique()\n",
    "    IpAddresses = adf[\"connection.resource_ip\"].unique()\n",
    "    return adf[\"resource.address\"].unique(),adf[\"connection.resource_ip\"].unique()\n",
    "\n",
    "# Polars alternative function\n",
    "def get_user_activity2(df,user):\n",
    "    logging.debug(\"getting all records for a given user.\")\n",
    "    return df.filter(pl.col(\"user.email\") == user)\n",
    "\n",
    "# Polars alternative function\n",
    "def get_user_client_ips2(df,user):\n",
    "    logging.debug(\"getting all records for a given user.\")\n",
    "    df1 = df.filter(pl.col(\"user.email\") == user)\n",
    "    return df1[\"connection.client_ip\"].unique()\n",
    "    \n",
    "# Polars alternative function\n",
    "def get_errors2(df):\n",
    "    return df.filter(pl.col(\"connection.error_message\") != \"NORMAL\")\n",
    "\n",
    "# Polars alternative function\n",
    "def get_failures2(df):\n",
    "    if \"event_type\" in df:\n",
    "        return df.filter(pl.col(\"event_type\") == \"failed_to_connect\")\n",
    "\n",
    "def get_relay_vs_p2p_traffic2(df):\n",
    "    p2p_events = df.filter(pl.col(\"relay.ips\").is_null())\n",
    "    relay_events = df.filter(pl.col(\"relay.ips\").is_not_null())\n",
    "    return p2p_events,relay_events\n",
    "\n",
    "\n",
    "################################################################\n",
    "\n",
    "# Ignore the following functions, they provide tooling for other useful functions\n",
    "\n",
    "################################################################\n",
    "\n",
    "def initiate_logging():\n",
    "    root = logging.getLogger()\n",
    "    root.setLevel(logging.INFO)\n",
    "    handler = logging.StreamHandler(sys.stdout)\n",
    "    handler.setLevel(logging.INFO)\n",
    "    formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "    handler.setFormatter(formatter)\n",
    "    root.addHandler(handler)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-fcVwLH2P41t"
   },
   "source": [
    "# Generate the final (XLSX) Report\n",
    "\n",
    "You can customize the output report below in many ways:\n",
    "\n",
    "* Change the title of each Tab\n",
    "* Activate or deactivate sections of the report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "C9VBI_V4Qbt4",
    "outputId": "2982d98e-d17b-48f7-a19e-2e2c9ea5585e",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting imported report to DataFrame\n",
      "Report has been successfully converted.\n",
      "extracting User Public IPs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/8f/2636np216p3bv65_5x2s4xjr0000gn/T/ipykernel_10914/3209044533.py:55: UserWarning: Comparisons with None always result in null. Consider using `.is_null()` or `.is_not_null()`.\n",
      "  df1 = df.filter(pl.col(\"user.email\") == user)\n",
      "/var/folders/8f/2636np216p3bv65_5x2s4xjr0000gn/T/ipykernel_10914/3209044533.py:50: UserWarning: Comparisons with None always result in null. Consider using `.is_null()` or `.is_not_null()`.\n",
      "  return df.filter(pl.col(\"user.email\") == user)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "extracting Error Report\n",
      "extracting Resource Matching Report\n",
      "extracting DNS Errors\n",
      "extracting Connection Errors\n",
      "extracting Overall Connector Stats\n",
      "extracting Per Connector Report\n",
      "writing IP information...\n",
      "writing activity summary...\n",
      "writing resource summary...\n",
      "writing resource match summary...\n",
      "wrinting DNS error summary...\n",
      "wrinting Connection error summary...\n",
      "writing connector summary...\n",
      "writing per Connector stats summary...\n",
      "Report Generation done. Here is how long it took to generate: 10241.491079330444ms\n"
     ]
    }
   ],
   "source": [
    "import polars as pl\n",
    "import time\n",
    "import json,requests,re\n",
    "from types import SimpleNamespace\n",
    "import datetime, time, logging, sys, base64, math\n",
    "from zoneinfo import ZoneInfo\n",
    "from urllib.request import urlopen\n",
    "from json import load\n",
    "from xlsxwriter import Workbook\n",
    "\n",
    "doUserIPInfo = True # Export User/IP Info\n",
    "userIPSummaryTitle = \"User IP Details\" # Title for Workbook Tab\n",
    "\n",
    "doUserActInfo = True # Export User Activity Details\n",
    "userActTitle = \"User Activity Details\" # Title for Workbook Tab\n",
    "\n",
    "doResInfo = True # Export Full Resource List\n",
    "doResTitle = \"Full Resource List\" # Title for Workbook Tab\n",
    "\n",
    "doMatchList = True # Export Match List for Wildcard Resources\n",
    "doMatchTitle = \"Resource Matching List\" # Title for Workbook Tab\n",
    "\n",
    "doConnectorsStats = True # Export Overall Connectors Stats\n",
    "doConnectorsStatsTitle = \"Connector Activities\" # Title for Workbook Tab\n",
    "\n",
    "doConnectorsStatsPerConnector = True # Export Per Connectors Stats\n",
    "\n",
    "doConnErrorsStats = True # Export Connection Error Stats\n",
    "doConnErrorsStatsTitle = \"Connection Errors\" # Title for Workbook Tab\n",
    "\n",
    "doDNSErrorsStats = True # Export DNS Error Stats\n",
    "doDNSErrorsStatsTitle = \"DNS Errors\" # Title for Workbook Tab\n",
    "\n",
    "# initiate logging (no need to change this)\n",
    "initiate_logging()\n",
    "startTime = time.time()\n",
    "\n",
    "# Convert entire report output to DataFrame for processing.\n",
    "print(\"Converting imported report to DataFrame\")\n",
    "df = convert_admin_console_report_to_df(full_console_report,local_tz)\n",
    "print(\"Report has been successfully converted.\")\n",
    "\n",
    "if (doUserIPInfo or doUserActInfo):\n",
    "    unique_users = df[\"user.email\"].unique()\n",
    "\n",
    "# End User Public IP tab\n",
    "if (doUserIPInfo):\n",
    "### Get User IP Info\n",
    "    print(\"extracting User Public IPs\")\n",
    "\n",
    "    user_ip_info_data = {\n",
    "        \"user\":[],\n",
    "        \"ipinfo\":[],\n",
    "    }\n",
    "    \n",
    "    user_ip_info_schema = {\n",
    "        \"user\":pl.String, \n",
    "        \"ipinfo\":pl.String\n",
    "    }\n",
    "\n",
    "    user_ip_info_df = pl.DataFrame(user_ip_info_data, schema=user_ip_info_schema)\n",
    "\n",
    "    for user in unique_users:\n",
    "        ips = get_user_client_ips2(df,user)\n",
    "        iplist = []\n",
    "        for ip in ips:\n",
    "            iplist.append(str(ip))\n",
    "            ip_info_for_user = \",\".join(iplist)\n",
    "\n",
    "        temp_ip_summary_data = {\n",
    "            \"user\": user, \n",
    "            \"ipinfo\":ip_info_for_user\n",
    "        }\n",
    "        \n",
    "        temp_ip_summary_df = pl.DataFrame(temp_ip_summary_data, schema=user_ip_info_schema)\n",
    "        user_ip_info_df = pl.concat([user_ip_info_df,temp_ip_summary_df])\n",
    "\n",
    "    user_ip_info_df = user_ip_info_df.rename(\n",
    "        {\n",
    "            \"user\":\"User Email\",\n",
    "            \"ipinfo\" : \"Public IPs\"\n",
    "        }\n",
    "    )\n",
    "\n",
    "# End User activity tab\n",
    "if (doUserActInfo):\n",
    "    user_activity_data = {\n",
    "        \"user\":[],\n",
    "        \"numconn\":[],\n",
    "        \"numerr\":[],\n",
    "        \"txbytes\":[],\n",
    "        \"rxbytes\":[],\n",
    "        \"p2p\":[],\n",
    "        \"relay\":[],\n",
    "        \"nbofdevices\":[],\n",
    "        \"lastactivity\":[]\n",
    "    }\n",
    "    user_activity_schema = {\n",
    "        \"user\":pl.String, \n",
    "        \"numconn\":pl.Int32, \n",
    "        \"numerr\":pl.Int32, \n",
    "        \"txbytes\":pl.Int64, \n",
    "        \"rxbytes\": pl.Int64, \n",
    "        \"p2p\":pl.Float32, \n",
    "        \"relay\":pl.Float32,\n",
    "        \"nbofdevices\":pl.Int32,\n",
    "        \"lastactivity\":pl.String\n",
    "    }\n",
    "    \n",
    "    user_activity_df = pl.DataFrame(user_activity_data, schema=user_activity_schema)\n",
    "    for user in unique_users:\n",
    "        user_act = get_user_activity2(df,user)\n",
    "        \n",
    "        # last user activity\n",
    "        lastactivity = user_act.tail(1)\n",
    "        lastactivitytimestamp = lastactivity['timestamp.readable']\n",
    "        \n",
    "        # list of active user devices\n",
    "        devices = user_act['device.id'].unique()\n",
    "        \n",
    "        # get all P2P & Relay events\n",
    "        p2p,relays = get_relay_vs_p2p_traffic2(user_act)\n",
    "        nb_p2p_events = len(p2p)\n",
    "        nb_relay_events = len(relays)\n",
    "        nb_total_events = nb_p2p_events + nb_relay_events\n",
    "        if nb_total_events > 0:\n",
    "            percentage_p2p = round(nb_p2p_events / nb_total_events * 100,2)\n",
    "            percentage_relay = round(nb_relay_events / nb_total_events * 100,2)\n",
    "        else:\n",
    "            percentage_p2p = 0\n",
    "            percentage_relay = 0\n",
    "            \n",
    "        # get all user errors\n",
    "        user_err = get_errors2(user_act)\n",
    "        \n",
    "        # get cummulative bandwidth data\n",
    "        total_tx = user_act['connection.tx'].sum()\n",
    "        total_rx = user_act['connection.rx'].sum()\n",
    "        \n",
    "        temp_act_data = {\n",
    "         \"user\": user, \n",
    "         \"numconn\":len(user_act), \n",
    "         \"numerr\": len(user_err), \n",
    "         \"txbytes\":total_tx, \n",
    "         \"rxbytes\":total_rx, \n",
    "         \"p2p\": percentage_p2p, \n",
    "         \"relay\": percentage_relay, \n",
    "         \"nbofdevices\": len(devices),\n",
    "         \"lastactivity\": lastactivitytimestamp\n",
    "        }\n",
    "        \n",
    "        temp_act_df = pl.DataFrame(temp_act_data, schema=user_activity_schema)\n",
    "        user_activity_df = pl.concat([user_activity_df, temp_act_df])\n",
    "\n",
    "    user_activity_df = user_activity_df.rename(\n",
    "        {\n",
    "            \"user\":\"User Email\",\n",
    "            \"numconn\" : \"Nb of Connections\",\n",
    "            \"numerr\" : \"Nb of Errors\",\n",
    "            \"txbytes\" : \"TX (bytes)\",\n",
    "            \"rxbytes\":\"RX (bytes)\",\n",
    "            \"p2p\":\"P2P Connections (%)\",\n",
    "            \"relay\":\"Relay Connections (%)\",\n",
    "            \"nbofdevices\":\"Number of Active Devices\",\n",
    "            \"lastactivity\": \"Last Activity\"\n",
    "        }\n",
    "    )\n",
    "\n",
    "# Overall Resource performance tab\n",
    "if (doResInfo):\n",
    "    all_errors = get_errors2(df)\n",
    "    unique_addr_in_df = df[\"resource.address\"].unique()\n",
    "    res_data = {\n",
    "        \"address\":[],\n",
    "        \"resourcedef\":[],\n",
    "        \"resourceids\":[],\n",
    "        \"rnetworks\":[],\n",
    "        \"numconn\":[],\n",
    "        \"numerrs\":[],\n",
    "        \"failrate\":[],\n",
    "        \"totaltx\":[],\n",
    "        \"totalrx\":[],\n",
    "        \"tcp\":[],\n",
    "        \"udp\":[],\n",
    "        \"icmp\":[]\n",
    "    }\n",
    "    \n",
    "    res_data_schema = {\n",
    "        \"address\":pl.String, \n",
    "        \"resourcedef\":pl.String, \n",
    "        \"resourceids\":pl.String, \n",
    "        \"rnetworks\":pl.String, \n",
    "        \"numconn\":pl.Int32, \n",
    "        \"numerrs\":pl.Int32, \n",
    "        \"failrate\":pl.Float32, \n",
    "        \"totaltx\":pl.Int64, \n",
    "        \"totalrx\":pl.Int64, \n",
    "        \"tcp\":pl.String, \n",
    "        \"udp\":pl.String, \n",
    "        \"icmp\":pl.String\n",
    "    }\n",
    "    \n",
    "    \n",
    "    \n",
    "    res_data_df = pl.DataFrame(res_data, schema=res_data_schema)\n",
    "    \n",
    "    print(\"extracting Error Report\")\n",
    "    ## for each address in the error list, check how many records are in error and what the percentage of errors is\n",
    "    \n",
    "    error_data_schema = {\n",
    "        \"address\":pl.String, \n",
    "        \"resourcedef\":pl.String, \n",
    "        \"resourceids\":pl.String, \n",
    "        \"rnetworks\":pl.String, \n",
    "        \"numconn\":pl.Int32, \n",
    "        \"numerrs\":pl.Int32, \n",
    "        \"failrate\":pl.Float32, \n",
    "        \"totaltx\":pl.Int64, \n",
    "        \"totalrx\":pl.Int64, \n",
    "        \"tcp\":pl.String, \n",
    "        \"udp\":pl.String, \n",
    "        \"icmp\":pl.String\n",
    "    }\n",
    "    \n",
    "    for addr in unique_addr_in_df:\n",
    "        if str(addr) != \"nan\":\n",
    "            total_for_addr = df.filter(pl.col(\"resource.address\") == addr)\n",
    "            err_for_addr = all_errors.filter(pl.col(\"resource.address\") == addr)\n",
    "\n",
    "            if len(total_for_addr) > 0:\n",
    "                percentage_failure = round(len(err_for_addr) / len(total_for_addr) * 100,2)\n",
    "            else:\n",
    "                percentage_failure = 100\n",
    "\n",
    "            port_connected = []\n",
    "            addr_activities = df.filter(pl.col(\"resource.address\") == addr)\n",
    "\n",
    "            # pull resource definition, resource ID (as would appear in the Admin Console & API) and Remote Network name\n",
    "            res_definitions = addr_activities[\"resource.applied_rule\"].unique()\n",
    "            res_ids = addr_activities[\"resource.id\"].unique()\n",
    "            rnetwork_names = addr_activities[\"remote_network.name\"].unique()\n",
    "\n",
    "            res_def_list=[]\n",
    "            for definition in res_definitions:\n",
    "                res_def_list.append(definition)\n",
    "            res_def_list_s=\";\".join(str(x) for x in res_def_list)\n",
    "            \n",
    "            res_id_list=[]\n",
    "            for definition in res_ids:\n",
    "                res_id_list.append(definition)\n",
    "            res_id_list_s=\";\".join(str(x) for x in res_id_list)\n",
    "            \n",
    "            res_rnetwork_list=[]\n",
    "            for definition in rnetwork_names:\n",
    "                res_rnetwork_list.append(definition)\n",
    "            res_rnetwork_list_s=\";\".join(str(x) for x in res_rnetwork_list)\n",
    "            \n",
    "            total_tx = addr_activities['connection.tx'].sum()\n",
    "            total_rx = addr_activities['connection.rx'].sum()\n",
    "\n",
    "            addr_activities_protocols = addr_activities[\"connection.protocol\"].unique()\n",
    "            addr_activities_icmp = addr_activities.filter(pl.col(\"connection.protocol\") == \"icmp\")\n",
    "            addr_activities_tcp = addr_activities.filter(pl.col(\"connection.protocol\") == \"tcp\")\n",
    "            addr_activities_udp = addr_activities.filter(pl.col(\"connection.protocol\") == \"udp\")\n",
    "\n",
    "            ports_tcp = addr_activities_tcp[\"connection.resource_port\"].unique()\n",
    "            ports_udp = addr_activities_udp[\"connection.resource_port\"].unique()\n",
    "            ports_icmp = addr_activities_icmp[\"connection.resource_port\"].unique()\n",
    "\n",
    "            tcp_port_list = []\n",
    "            udp_port_list = []\n",
    "            icmp_port_list = []\n",
    "            for port in ports_tcp:\n",
    "                tcp_port_list.append(port)\n",
    "            for port in ports_udp:\n",
    "                udp_port_list.append(port)\n",
    "            for port in ports_icmp:\n",
    "                icmp_port_list.append(port)\n",
    "            tcp_port_list_s=\";\".join(str(x) for x in tcp_port_list)\n",
    "            udp_port_list_s=\";\".join(str(x) for x in udp_port_list)\n",
    "            icmp_port_list_s=\";\".join(str(x) for x in icmp_port_list)\n",
    "            \n",
    "            res_data_temp_df = pl.DataFrame(res_data, schema=error_data_schema)\n",
    "\n",
    "\n",
    "            temp_res_data = {\n",
    "                \"address\":str(addr),\n",
    "                \"resourcedef\":str(res_def_list_s),\n",
    "                \"resourceids\":str(res_id_list_s),\n",
    "                \"rnetworks\":str(res_rnetwork_list_s), \n",
    "                \"numconn\":len(total_for_addr), \n",
    "                \"numerrs\":len(err_for_addr), \n",
    "                \"failrate\":percentage_failure, \n",
    "                \"totaltx\":total_tx, \n",
    "                \"totalrx\":total_rx, \n",
    "                \"tcp\":tcp_port_list_s, \n",
    "                \"udp\":udp_port_list_s, \n",
    "                \"icmp\":icmp_port_list_s\n",
    "            }\n",
    "            \n",
    "            temp_res_df = pl.DataFrame(temp_res_data, schema=error_data_schema)\n",
    "            res_data_df = pl.concat([res_data_df, temp_res_df])\n",
    "\n",
    "    res_data_df = res_data_df.rename(\n",
    "        {\n",
    "            \"address\":\"Resource Address\",\n",
    "            \"resourcedef\":\"Resource Definitions\",\n",
    "            \"resourceids\":\"Resource IDs\",\n",
    "            \"rnetworks\":\"Remote Networks\",\n",
    "            \"numconn\" : \"Nb of Connections\",\n",
    "            \"numerrs\" : \"Nb of Errors\",\n",
    "            \"totaltx\" : \"TX (bytes)\",\n",
    "            \"totalrx\":\"RX (bytes)\",\n",
    "            \"failrate\":\"Failure Rate (%)\"\n",
    "        }\n",
    "    )\n",
    "\n",
    "# Resource matching list\n",
    "if (doMatchList):\n",
    "    # get the list of unique resource definitions that have caught traffic (with the list of actual addresses hit for each resource)\n",
    "    print(\"extracting Resource Matching Report\")\n",
    "    unique_resources = df[\"resource.applied_rule\"].unique()\n",
    "    \n",
    "    match_data = {\n",
    "        \"resource\":[],\n",
    "        \"matches\":[]\n",
    "    }\n",
    "    \n",
    "    match_data_schema = {\n",
    "        \"resource\":pl.String, \n",
    "        \"matches\":pl.String\n",
    "    }\n",
    "    \n",
    "    match_data_df = pl.DataFrame(match_data, schema=match_data_schema)\n",
    "    for r in unique_resources:\n",
    "        addresses_for_resource,ips_for_resource = get_endpoints_from_resource_activity2(df,r)\n",
    "        \n",
    "        # if the list of unique addresses returns only 1 address and said address is None, then the source of truth should be IPs and not FQDNs\n",
    "        if len(addresses_for_resource) == 1:\n",
    "            addr_joined=\"; \".join(str(x) for x in addresses_for_resource)\n",
    "            if addr_joined == \"None\":\n",
    "                addr_joined=\"; \".join(str(x) for x in ips_for_resource)\n",
    "        else:\n",
    "            addr_joined=\"; \".join(str(x) for x in addresses_for_resource) \n",
    "                \n",
    "        temp_match_data = {\n",
    "            \"resource\":r, \n",
    "            \"matches\":addr_joined\n",
    "        }\n",
    "        \n",
    "        temp_match_data_df = pl.DataFrame(temp_match_data, schema=match_data_schema)\n",
    "        match_data_df = pl.concat([match_data_df, temp_match_data_df])\n",
    "\n",
    "    match_data_df = match_data_df.rename(\n",
    "        {\n",
    "            \"resource\":\"Resource Definition\",\n",
    "            \"matches\" : \"Corresponding Addresses\"\n",
    "        }\n",
    "    )\n",
    "\n",
    "# DNS Error tab\n",
    "if(doDNSErrorsStats):\n",
    "    print(\"extracting DNS Errors\")\n",
    "\n",
    "    connact_data = {\n",
    "            \"address\":[],\n",
    "            \"nberrors\":[]\n",
    "      }\n",
    "    \n",
    "    connact_data_schema = {\n",
    "        \"address\":pl.String, \n",
    "        \"nberrors\":pl.Int64\n",
    "    }\n",
    "\n",
    "    dns_errors_df = pl.DataFrame(connact_data, schema=connact_data_schema)\n",
    "    activities_dnserror = df.filter(pl.col(\"connection.error_message\") == \"DNS_ERROR\")\n",
    "    unique_dns_failures = activities_dnserror['resource.address'].unique()\n",
    "\n",
    "    for addr in unique_dns_failures:\n",
    "        dns_ct = activities_dnserror.filter(pl.col(\"resource.address\") == addr).select(pl.len()).item()\n",
    "\n",
    "        temp_dns_errors = {\n",
    "            \"address\":addr, \n",
    "            \"nberrors\":dns_ct\n",
    "        }\n",
    "        \n",
    "        temp_dns_errors_df = pl.DataFrame(temp_dns_errors, schema=connact_data_schema)\n",
    "\n",
    "        dns_errors_df = pl.concat([dns_errors_df, temp_dns_errors_df])\n",
    "\n",
    "    dns_errors_df = dns_errors_df.rename(\n",
    "        {\n",
    "            \"address\":\"Resource Address\",\n",
    "            \"nberrors\" : \"Number of DNS Errors\"\n",
    "        }\n",
    "    )\n",
    "\n",
    "# Connection Error tab\n",
    "if(doConnErrorsStats):\n",
    "    print(\"extracting Connection Errors\")\n",
    "\n",
    "    connact_data = {\n",
    "        \"address\":[],\n",
    "        \"nberrors\":[]\n",
    "    }\n",
    "    \n",
    "    connact_data_schema = {\n",
    "        \"address\":pl.String, \n",
    "        \"nberrors\":pl.Int64\n",
    "    }\n",
    "\n",
    "    conn_errors_df = pl.DataFrame(connact_data, schema=connact_data_schema)\n",
    "    activities_connectionfailed = df.filter(pl.col(\"connection.error_message\") == \"CONNECTION_FAILED\")\n",
    "    unique_address_failures = activities_connectionfailed['resource.address'].unique()\n",
    "\n",
    "    for addr in unique_address_failures:\n",
    "        err_ct = activities_connectionfailed.filter(pl.col(\"resource.address\") == addr).select(pl.len()).item()\n",
    "\n",
    "        temp_conn_errors = {\n",
    "            \"address\":addr, \n",
    "            \"nberrors\":err_ct\n",
    "        }\n",
    "        \n",
    "        temp_conn_errors_df = pl.DataFrame(temp_conn_errors, schema=connact_data_schema)\n",
    "        conn_errors_df = pl.concat([conn_errors_df, temp_conn_errors_df])\n",
    "\n",
    "    conn_errors_df = conn_errors_df.rename(\n",
    "        {\n",
    "            \"address\":\"Resource Address\",\n",
    "            \"nberrors\" : \"Number of Connection Errors\"\n",
    "        }\n",
    "    )\n",
    "\n",
    "# Overall Connector performance tab\n",
    "if (doConnectorsStats):\n",
    "    print(\"extracting Overall Connector Stats\")\n",
    "    unique_connectors = df[\"connector.name\"].unique()\n",
    "    \n",
    "    connact_data = {\n",
    "        \"connectorname\":[],\n",
    "        \"rnname\":[],\n",
    "        \"totalactivities\":[],\n",
    "        \"connsuccess\":[],\n",
    "        \"connerror\":[],\n",
    "        \"dnserror\":[]\n",
    "    }\n",
    "    \n",
    "    connact_data_schema = {\n",
    "        \"connectorname\":pl.String,\n",
    "        \"rnname\":pl.String, \n",
    "        \"totalactivities\":pl.Int64, \n",
    "        \"connsuccess\":pl.Int64, \n",
    "        \"connerror\":pl.Int64, \n",
    "        \"dnserror\":pl.Int64\n",
    "    }\n",
    "    \n",
    "    connector_data_df = pl.DataFrame(connact_data, schema=connact_data_schema)\n",
    "    \n",
    "    for connector in unique_connectors:\n",
    "        \n",
    "        conn_activities = df.filter(pl.col(\"connector.name\") == connector)\n",
    "        rn_name = conn_activities['remote_network.name'].unique()[0]\n",
    "        conn_activities_normal = conn_activities.filter(pl.col(\"connection.error_message\") == \"NORMAL\")\n",
    "        conn_activities_connectionfailed = conn_activities.filter(pl.col(\"connection.error_message\") == \"CONNECTION_FAILED\")\n",
    "        conn_activities_dnserror = conn_activities.filter(pl.col(\"connection.error_message\") == \"DNS_ERROR\")\n",
    "\n",
    "        temp_res_data = {\n",
    "            \"connectorname\":str(connector),\n",
    "            \"rnname\":str(rn_name), \n",
    "            \"totalactivities\":len(conn_activities),\n",
    "            \"connsuccess\":len(conn_activities_normal),\n",
    "            \"connerror\":len(conn_activities_connectionfailed),\n",
    "            \"dnserror\":len(conn_activities_dnserror)\n",
    "        }\n",
    "        temp_res_df = pl.DataFrame(temp_res_data, schema=connact_data_schema)\n",
    "\n",
    "        connector_data_df = pl.concat([connector_data_df, temp_res_df])\n",
    "\n",
    "    connector_data_df = connector_data_df.rename(\n",
    "        {\n",
    "            \"connectorname\":\"Connector Name\",\n",
    "            \"totalactivities\" : \"Nb of Activities\",\n",
    "            \"rnname\":\"Remote Network Name\",\n",
    "            \"connsuccess\":\"Successful Connections\",\n",
    "            \"connerror\":\"Failed Connections\",\n",
    "            \"dnserror\":\"DNS Errors\"\n",
    "        }\n",
    "    )\n",
    "\n",
    "# Per Connector performance tab\n",
    "if (doConnectorsStatsPerConnector):\n",
    "    print(\"extracting Per Connector Report\")\n",
    "    unique_connectors = df[\"connector.name\"].unique()\n",
    "    \n",
    "    connact_data = {\n",
    "        \"date\":[],\n",
    "        \"connectorname\":[],\n",
    "        \"rnname\":[],\n",
    "        \"totalactivities\":[],\n",
    "        \"connsuccess\":[],\n",
    "        \"connerror\":[],\n",
    "        \"dnserror\":[]\n",
    "    }\n",
    "    \n",
    "    connact_data_scheme = {\n",
    "        \"date\":pl.String,\n",
    "        \"connectorname\":pl.String,\n",
    "        \"rnname\":pl.String, \n",
    "        \"totalactivities\":pl.Int64, \n",
    "        \"connsuccess\":pl.Int64, \n",
    "        \"connerror\":pl.Int64, \n",
    "        \"dnserror\":pl.Int64\n",
    "    }\n",
    "\n",
    "    all_connector_data = {}\n",
    "    for connector in unique_connectors:\n",
    "        per_connector_data_df = pl.DataFrame(connact_data, schema=connact_data_scheme)\n",
    "        conn_activities = df.filter(pl.col(\"connector.name\") == connector)\n",
    "        rn_name = conn_activities['remote_network.name'].unique()[0]\n",
    "        unique_days_of_Activities = conn_activities[\"timestamp.yymmdd\"].unique()\n",
    "        for day in unique_days_of_Activities:\n",
    "            conn_day_activities = conn_activities.filter(pl.col(\"timestamp.yymmdd\") == day)\n",
    "            connection_normal = conn_day_activities.filter(pl.col(\"connection.error_message\") == \"NORMAL\")\n",
    "            connection_errors = conn_day_activities.filter(pl.col(\"connection.error_message\") == \"CONNECTION_FAILED\")\n",
    "            dns_errors = conn_day_activities.filter(pl.col(\"connection.error_message\") == \"DNS_ERROR\")\n",
    "\n",
    "            temp_res_data = {\n",
    "                \"date\":str(day),\n",
    "                \"connectorname\":str(connector),\n",
    "                \"rnname\":str(rn_name), \n",
    "                \"totalactivities\":len(conn_day_activities),\n",
    "                \"connsuccess\":len(connection_normal),\n",
    "                \"connerror\":len(connection_errors),\n",
    "                \"dnserror\":len(dns_errors)\n",
    "            }\n",
    "\n",
    "            temp_res_df = pl.DataFrame(temp_res_data, schema=connact_data_scheme)\n",
    "\n",
    "            per_connector_data_df = pl.concat([per_connector_data_df, temp_res_df])\n",
    "\n",
    "        per_connector_data_df = per_connector_data_df.rename(\n",
    "            {\n",
    "                \"connectorname\":\"Connector Name\",\n",
    "                \"totalactivities\" : \"Nb of Activities\",\n",
    "                \"rnname\":\"Remote Network Name\",\n",
    "                \"connsuccess\":\"Successful Connections\",\n",
    "                \"connerror\":\"Failed Connections\",\n",
    "                \"dnserror\":\"DNS Errors\"\n",
    "            }\n",
    "        )\n",
    "\n",
    "        all_connector_data[connector] = per_connector_data_df\n",
    "\n",
    "# Generate the XLSX Report with all tabs & data\n",
    "with Workbook(workbookName) as wb:\n",
    "\n",
    "    if(doUserIPInfo):\n",
    "        print(\"writing IP information...\")\n",
    "        user_ip_info_df.write_excel(\n",
    "            workbook=wb,\n",
    "            worksheet=userIPSummaryTitle\n",
    "        )\n",
    "    if(doUserActInfo):\n",
    "        print(\"writing activity summary...\")\n",
    "        user_activity_df.write_excel(\n",
    "            workbook=wb,\n",
    "            worksheet=userActTitle\n",
    "        )\n",
    "\n",
    "    if(doResInfo):\n",
    "        print(\"writing resource summary...\")\n",
    "        res_data_df.write_excel(\n",
    "            workbook=wb,\n",
    "            worksheet=doResTitle\n",
    "        )\n",
    "    if(doMatchList):\n",
    "        print(\"writing resource match summary...\")\n",
    "        match_data_df.write_excel(\n",
    "            workbook=wb,\n",
    "            worksheet=doMatchTitle\n",
    "        )\n",
    "    if(doDNSErrorsStats):\n",
    "        print(\"wrinting DNS error summary...\")\n",
    "        dns_errors_df.write_excel(\n",
    "            workbook=wb,\n",
    "            worksheet=doDNSErrorsStatsTitle\n",
    "      )\n",
    "\n",
    "    if(doConnErrorsStats):\n",
    "        print(\"wrinting Connection error summary...\")\n",
    "        conn_errors_df.write_excel(\n",
    "            workbook=wb,\n",
    "            worksheet=doConnErrorsStatsTitle\n",
    "      )\n",
    "    if(doConnectorsStats):\n",
    "        print(\"writing connector summary...\")\n",
    "        connector_data_df.write_excel(\n",
    "        workbook=wb,\n",
    "        worksheet=doConnectorsStatsTitle\n",
    "    )\n",
    "\n",
    "    if(doConnectorsStatsPerConnector):\n",
    "        print(\"writing per Connector stats summary...\")\n",
    "        #all_connector_data\n",
    "        for key in all_connector_data:\n",
    "            all_connector_data[key].write_excel(\n",
    "                workbook=wb,\n",
    "                worksheet=key\n",
    "            )\n",
    "\n",
    "    for WS in wb.worksheets():\n",
    "        WS.autofit()\n",
    "        WS.autofilter('A1:Z1000')\n",
    "\n",
    "    endTime = time.time()\n",
    "    timeDiff = endTime - startTime\n",
    "    timeDiff = timeDiff * 10**3\n",
    "    print(\"Report Generation done. Here is how long it took to generate: \" + str(timeDiff) + \"ms\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
